"""
OpenAI SDKå°è£…å±‚

æä¾›ç»Ÿä¸€çš„OpenAI SDKå¼‚æ­¥æ¥å£ï¼Œé›†æˆé…ç½®ç®¡ç†ã€é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶å’ŒFunction CallingåŠŸèƒ½ã€‚
"""

import asyncio
import os
import time
from typing import Dict, List, Any, Optional, AsyncIterator, Callable, TypedDict
import copy
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk

from utils.logger_config import get_openai_logger

try:
    from dynaconf import Dynaconf
    DYNACONF_AVAILABLE = True
except ImportError:
    DYNACONF_AVAILABLE = False


class Message(TypedDict):
    """æ¶ˆæ¯ç±»å‹å®šä¹‰"""
    role: str
    content: str


class ToolCallTagFilter:
    """
    å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤å™¨å’Œè½¬æ¢å™¨ï¼ˆçŠ¶æ€æœºæ¨¡å¼ï¼‰

    ä½¿ç”¨çŠ¶æ€æœºæ¨¡å¼å¤„ç†è·¨chunkçš„å·¥å…·è°ƒç”¨æ ‡ç­¾åˆ†å‰²æƒ…å†µï¼Œ
    æ”¯æŒæ ‡ç­¾è¢«ä»»æ„åˆ†å‰²çš„è¾¹ç•Œæƒ…å†µï¼ˆå¦‚ chunk1="<tool_", chunk2="call>{"name""ï¼‰
    """

    def __init__(self):
        # çŠ¶æ€æœºçŠ¶æ€
        self.state = "NORMAL"  # NORMAL, COLLECTING_START_TAG, IN_TOOL_CALL, COLLECTING_END_TAG

        # æ ‡ç­¾å®šä¹‰
        self.start_tag = "<tool_call>"
        self.end_tag = "</tool_call>"

        # æ ‡ç­¾æ”¶é›†ç¼“å†²åŒº
        self.tag_buffer = ""
        self.tag_target = ""

        # å†…å®¹ç¼“å†²åŒº
        self.content_buffer = ""
        self.tool_call_content = ""

        # è¾“å‡ºç¼“å†²åŒº
        self.output_buffer = ""

        # æå–çš„å·¥å…·è°ƒç”¨
        self.extracted_tool_calls = []

    def process_chunk(self, content: str) -> str:
        """
        å¤„ç†æµå¼å†…å®¹å—ï¼Œä½¿ç”¨çŠ¶æ€æœºæ¨¡å¼è¿‡æ»¤å·¥å…·è°ƒç”¨æ ‡ç­¾

        Args:
            content: åŸå§‹å†…å®¹å—

        Returns:
            è¿‡æ»¤åçš„å¯æ˜¾ç¤ºå†…å®¹
        """
        if not content:
            return ""

        output = ""

        for char in content:
            if self.state == "NORMAL":
                output += self._process_normal_char(char)
            elif self.state == "COLLECTING_START_TAG":
                output += self._process_start_tag_char(char)
            elif self.state == "IN_TOOL_CALL":
                self._process_tool_call_char(char)
            elif self.state == "COLLECTING_END_TAG":
                self._process_end_tag_char(char)

        return output

    def _process_normal_char(self, char: str) -> str:
        """å¤„ç†æ­£å¸¸çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        if char == '<':
            # å¼€å§‹æ”¶é›†å¼€å§‹æ ‡ç­¾
            self.state = "COLLECTING_START_TAG"
            self.tag_buffer = '<'
            self.tag_target = self.start_tag
            return ""  # ä¸è¾“å‡ºï¼Œç­‰å¾…ç¡®è®¤æ˜¯å¦ä¸ºå·¥å…·è°ƒç”¨æ ‡ç­¾
        else:
            return char

    def _process_start_tag_char(self, char: str) -> str:
        """å¤„ç†å¼€å§‹æ ‡ç­¾æ”¶é›†çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        self.tag_buffer += char

        if len(self.tag_buffer) <= len(self.tag_target):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ ‡ç­¾
            if self.tag_buffer == self.tag_target[:len(self.tag_buffer)]:
                if self.tag_buffer == self.tag_target:
                    # å®Œæ•´åŒ¹é…å¼€å§‹æ ‡ç­¾
                    self.state = "IN_TOOL_CALL"
                    self.tool_call_content = ""
                    self.tag_buffer = ""
                    return ""  # ä¸è¾“å‡ºæ ‡ç­¾
                else:
                    # éƒ¨åˆ†åŒ¹é…ï¼Œç»§ç»­æ”¶é›†
                    return ""
            else:
                # ä¸åŒ¹é…ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹å¹¶å›åˆ°æ­£å¸¸çŠ¶æ€
                output = self.tag_buffer
                self.state = "NORMAL"
                self.tag_buffer = ""
                return output
        else:
            # è¶…å‡ºæ ‡ç­¾é•¿åº¦ï¼Œä¸åŒ¹é…ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹å¹¶å›åˆ°æ­£å¸¸çŠ¶æ€
            output = self.tag_buffer
            self.state = "NORMAL"
            self.tag_buffer = ""
            return output

    def _process_tool_call_char(self, char: str):
        """å¤„ç†å·¥å…·è°ƒç”¨å†…å®¹çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        if char == '<':
            # å¯èƒ½æ˜¯ç»“æŸæ ‡ç­¾çš„å¼€å§‹
            self.state = "COLLECTING_END_TAG"
            self.tag_buffer = '<'
            self.tag_target = self.end_tag
        else:
            self.tool_call_content += char

    def _process_end_tag_char(self, char: str):
        """å¤„ç†ç»“æŸæ ‡ç­¾æ”¶é›†çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        self.tag_buffer += char

        if len(self.tag_buffer) <= len(self.tag_target):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ ‡ç­¾
            if self.tag_buffer == self.tag_target[:len(self.tag_buffer)]:
                if self.tag_buffer == self.tag_target:
                    # å®Œæ•´åŒ¹é…ç»“æŸæ ‡ç­¾ï¼Œå®Œæˆå·¥å…·è°ƒç”¨æå–
                    self._parse_and_store_tool_call(self.tool_call_content)
                    self.state = "NORMAL"
                    self.tag_buffer = ""
                    self.tool_call_content = ""
                # éƒ¨åˆ†åŒ¹é…ï¼Œç»§ç»­æ”¶é›†
            else:
                # ä¸åŒ¹é…ï¼Œå°†ç¼“å†²çš„å†…å®¹åŠ å…¥å·¥å…·è°ƒç”¨å†…å®¹ï¼Œç»§ç»­æ”¶é›†å·¥å…·è°ƒç”¨
                self.tool_call_content += self.tag_buffer
                self.state = "IN_TOOL_CALL"
                self.tag_buffer = ""
        else:
            # è¶…å‡ºæ ‡ç­¾é•¿åº¦ï¼Œä¸åŒ¹é…ï¼Œå°†ç¼“å†²çš„å†…å®¹åŠ å…¥å·¥å…·è°ƒç”¨å†…å®¹
            self.tool_call_content += self.tag_buffer
            self.state = "IN_TOOL_CALL"
            self.tag_buffer = ""

    def finalize(self) -> str:
        """
        å®Œæˆå¤„ç†ï¼Œè¿”å›å‰©ä½™çš„å¯æ˜¾ç¤ºå†…å®¹

        Returns:
            å‰©ä½™çš„å¯æ˜¾ç¤ºå†…å®¹
        """
        output = ""

        # å¤„ç†æœªå®Œæˆçš„çŠ¶æ€
        if self.state == "COLLECTING_START_TAG":
            # æœªå®Œæˆçš„å¼€å§‹æ ‡ç­¾æ”¶é›†ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹
            output += self.tag_buffer
        elif self.state == "IN_TOOL_CALL":
            # æœªå®Œæˆçš„å·¥å…·è°ƒç”¨ï¼Œä¸è¾“å‡ºï¼ˆå·¥å…·è°ƒç”¨ä¸å®Œæ•´ï¼‰
            pass
        elif self.state == "COLLECTING_END_TAG":
            # æœªå®Œæˆçš„ç»“æŸæ ‡ç­¾æ”¶é›†ï¼Œå°†ç¼“å†²å†…å®¹ä½œä¸ºå·¥å…·è°ƒç”¨å†…å®¹çš„ä¸€éƒ¨åˆ†
            # ä½†ç”±äºå·¥å…·è°ƒç”¨æœªå®Œæˆï¼Œä¸è¾“å‡º
            pass

        # é‡ç½®çŠ¶æ€
        self.state = "NORMAL"
        self.tag_buffer = ""
        self.tool_call_content = ""

        return output

    def _parse_and_store_tool_call(self, tool_call_content: str) -> None:
        """
        è§£æå·¥å…·è°ƒç”¨å†…å®¹å¹¶å­˜å‚¨ä¸ºæ ‡å‡†æ ¼å¼

        Args:
            tool_call_content: å·¥å…·è°ƒç”¨çš„JSONå†…å®¹
        """
        import json
        import uuid

        try:
            # è§£æJSONå†…å®¹
            tool_call_data = json.loads(tool_call_content)

            # éªŒè¯å¿…éœ€å­—æ®µ
            if "name" not in tool_call_data:
                return

            # ç”Ÿæˆå”¯ä¸€çš„call_id
            call_id = f"call_{uuid.uuid4().hex[:8]}"

            # ç¡®ä¿argumentsæ˜¯å­—ç¬¦ä¸²æ ¼å¼
            arguments = tool_call_data.get("arguments", {})
            if isinstance(arguments, dict):
                arguments_str = json.dumps(arguments, ensure_ascii=False)
            else:
                arguments_str = str(arguments)

            # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„å·¥å…·è°ƒç”¨
            standard_tool_call = {
                "id": call_id,
                "type": "function",
                "function": {
                    "name": tool_call_data["name"],
                    "arguments": arguments_str
                }
            }

            self.extracted_tool_calls.append(standard_tool_call)

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            # è§£æå¤±è´¥ï¼Œå¿½ç•¥è¿™ä¸ªå·¥å…·è°ƒç”¨
            pass

    def get_extracted_tool_calls(self) -> list:
        """
        è·å–æå–çš„å·¥å…·è°ƒç”¨åˆ—è¡¨

        Returns:
            æ ‡å‡†æ ¼å¼çš„å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        return self.extracted_tool_calls.copy()




class SimpleOpenAIConfig:
    """ç®€åŒ–çš„OpenAIé…ç½®ç±»"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "gpt-4",
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        timeout: float = 120.0,
        max_retries: int = 3,
        retry_delay: float = 2.0,
        log_requests: bool = True,
        log_responses: bool = True,
        function_calling_enabled: bool = True,
        tool_choice: str = "auto",
        # æ–°å¢ï¼šç»†ç²’åº¦é‡è¯•é…ç½®
        retry_config: Optional[Dict[str, Any]] = None,
    ):
        # å°è¯•ä» settings.toml åŠ è½½é…ç½®
        settings = self._load_settings()

        self.api_key = api_key or self._get_setting(settings, "llm.api_key") or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or self._get_setting(settings, "llm.base_url") or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model = self._get_setting(settings, "llm.model") or os.getenv("OPENAI_MODEL", model)
        self.temperature = self._get_setting(settings, "llm.temperature", temperature)
        self.max_tokens = self._get_setting(settings, "llm.max_tokens", max_tokens)
        self.timeout = self._get_setting(settings, "llm.timeout", timeout)
        self.max_retries = self._get_setting(settings, "llm.max_retries", max_retries)
        self.retry_delay = self._get_setting(settings, "llm.retry_delay", retry_delay)
        self.log_requests = self._get_setting(settings, "llm.log_requests", log_requests)
        self.log_responses = self._get_setting(settings, "llm.log_responses", log_responses)
        self.function_calling_enabled = self._get_setting(settings, "llm.function_calling_enabled", function_calling_enabled)
        self.tool_choice = self._get_setting(settings, "llm.tool_choice", tool_choice)
        
        # æ–°å¢ï¼šç»†ç²’åº¦é‡è¯•é…ç½®
        self.retry_config = retry_config or self._get_setting(settings, "llm.retry_config", self._get_default_retry_config())

        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or configure llm.api_key in settings.toml.")

    def _load_settings(self):
        """åŠ è½½ Dynaconf è®¾ç½®"""
        if not DYNACONF_AVAILABLE:
            return None

        try:
            return Dynaconf(
                settings_files=["settings.toml", "settings.local.toml", ".secrets.toml"],
                environments=True,
                env_switcher="ENV_FOR_DYNACONF",
                load_dotenv=True,
            )
        except Exception:
            return None

    def _get_setting(self, settings, key: str, default: Any = None) -> Any:
        """ä»è®¾ç½®ä¸­è·å–å€¼"""
        if settings is None:
            return default

        try:
            return settings.get(key, default)
        except Exception:
            return default

    def to_openai_client_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºOpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å‚æ•°"""
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
            "timeout": self.timeout,
            "max_retries": self.max_retries,
        }

    def to_chat_completion_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºchat completionè°ƒç”¨å‚æ•°"""
        kwargs = {
            "model": self.model,
            "temperature": self.temperature,
        }

        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        return kwargs

    def _get_default_retry_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤çš„é‡è¯•é…ç½®"""
        return {
            # ä¸åŒé”™è¯¯ç±»å‹çš„æœ€å¤§é‡è¯•æ¬¡æ•°
            "max_retries_by_error_type": {
                "rate_limit": 5,      # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼šæœ€å¤šé‡è¯•5æ¬¡
                "timeout": 3,         # è¶…æ—¶é”™è¯¯ï¼šæœ€å¤šé‡è¯•3æ¬¡  
                "network": 3,         # ç½‘ç»œé”™è¯¯ï¼šæœ€å¤šé‡è¯•3æ¬¡
                "server_error": 2,     # æœåŠ¡å™¨é”™è¯¯ï¼šæœ€å¤šé‡è¯•2æ¬¡
                "default": 3          # é»˜è®¤é‡è¯•æ¬¡æ•°
            },
            # ä¸åŒé”™è¯¯ç±»å‹çš„åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰
            "base_delay_by_error_type": {
                "rate_limit": 5.0,    # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼šåˆå§‹å»¶è¿Ÿ5ç§’
                "timeout": 2.0,       # è¶…æ—¶é”™è¯¯ï¼šåˆå§‹å»¶è¿Ÿ2ç§’
                "network": 1.0,       # ç½‘ç»œé”™è¯¯ï¼šåˆå§‹å»¶è¿Ÿ1ç§’
                "server_error": 3.0,  # æœåŠ¡å™¨é”™è¯¯ï¼šåˆå§‹å»¶è¿Ÿ3ç§’
                "default": 2.0        # é»˜è®¤åˆå§‹å»¶è¿Ÿ
            },
            # æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            "max_delay": 60.0,
            # æ˜¯å¦å¯ç”¨jitterï¼ˆéšæœºæŠ–åŠ¨ï¼‰
            "enable_jitter": True,
            # jitterèŒƒå›´ï¼ˆ0.0-1.0ï¼Œè¡¨ç¤ºÂ±ç™¾åˆ†æ¯”ï¼‰
            "jitter_range": 0.25,
            # å¯é‡è¯•çš„é”™è¯¯ç±»å‹æ¨¡å¼
            "retryable_error_patterns": {
                "rate_limit": ["rate_limit", "429", "quota", "limit"],
                "timeout": ["timeout", "timed out", "time out"],
                "network": ["connection", "network", "dns", "ssl", "socket"],
                "server_error": ["server_error", "500", "502", "503", "504", "internal"]
            }
        }


class OpenAIClientError(Exception):
    """OpenAIå®¢æˆ·ç«¯é”™è¯¯åŸºç±»"""
    
    def __init__(self, message: str, original_error: Optional[Exception] = None, 
                 error_type: str = "unknown", request_info: Optional[Dict[str, Any]] = None):
        super().__init__(message)
        self.original_error = original_error
        self.error_type = error_type
        self.request_info = request_info or {}
        self.timestamp = time.time()
        
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œä¾¿äºåºåˆ—åŒ–å’Œæ—¥å¿—è®°å½•"""
        return {
            "message": str(self),
            "error_type": self.error_type,
            "original_error": str(self.original_error) if self.original_error else None,
            "request_info": self.request_info,
            "timestamp": self.timestamp
        }


class OpenAIRateLimitError(OpenAIClientError):
    """APIé€Ÿç‡é™åˆ¶é”™è¯¯"""
    
    def __init__(self, message: str, original_error: Optional[Exception] = None, 
                 request_info: Optional[Dict[str, Any]] = None, 
                 retry_after: Optional[float] = None):
        super().__init__(message, original_error, "rate_limit", request_info)
        self.retry_after = retry_after
        
    def to_dict(self) -> Dict[str, Any]:
        result = super().to_dict()
        result["retry_after"] = self.retry_after
        return result


class OpenAITimeoutError(OpenAIClientError):
    """APIè¶…æ—¶é”™è¯¯"""
    
    def __init__(self, message: str, original_error: Optional[Exception] = None, 
                 request_info: Optional[Dict[str, Any]] = None):
        super().__init__(message, original_error, "timeout", request_info)


class OpenAIRetryableError(OpenAIClientError):
    """å¯é‡è¯•çš„APIé”™è¯¯"""
    
    def __init__(self, message: str, original_error: Optional[Exception] = None, 
                 request_info: Optional[Dict[str, Any]] = None, 
                 error_type: str = "retryable"):
        super().__init__(message, original_error, error_type, request_info)


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0, 
                 retry_config: Optional[Dict[str, Any]] = None, client: Optional[Any] = None):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.retry_config = retry_config or {}
        self.client = client  # OpenAIClientå®ä¾‹å¼•ç”¨
        
        # åˆå¹¶é»˜è®¤é…ç½®
        self._merge_default_config()

    async def execute_with_retry(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œå‡½æ•°å¹¶åœ¨å¤±è´¥æ—¶é‡è¯•

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)

            except Exception as e:
                last_error = e

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(e, attempt):
                    break

                # åˆ†ç±»é”™è¯¯ç±»å‹
                error_type = self._classify_error_type(e)
                
                # è®¡ç®—å»¶è¿Ÿæ—¶é—´ï¼ˆåŸºäºé”™è¯¯ç±»å‹ï¼‰
                delay = self._calculate_delay(attempt, error_type)

                # ä½¿ç”¨æ—¥å¿—è®°å½•é‡è¯•ä¿¡æ¯
                from utils.logger_config import get_logger
                logger = get_logger("retry_manager")
                logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self._get_max_retries_for_error_type(error_type)}): {e}")
                logger.info(f"ğŸ”„ é”™è¯¯ç±»å‹: {error_type}, ç­‰å¾… {delay:.1f}ç§’åé‡è¯•...")

                # æ›´æ–°é‡è¯•ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                self._update_retry_stats(error_type, False)

                await asyncio.sleep(delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise last_error

    def _merge_default_config(self) -> None:
        """åˆå¹¶é»˜è®¤é…ç½®"""
        default_config = {
            "max_retries_by_error_type": {
                "rate_limit": 5,
                "timeout": 3,
                "network": 3,
                "server_error": 2,
                "default": 3
            },
            "base_delay_by_error_type": {
                "rate_limit": 5.0,
                "timeout": 2.0,
                "network": 1.0,
                "server_error": 3.0,
                "default": 2.0
            },
            "max_delay": 60.0,
            "enable_jitter": True,
            "jitter_range": 0.25,
            "retryable_error_patterns": {
                "rate_limit": ["rate_limit", "429", "quota", "limit"],
                "timeout": ["timeout", "timed out", "time out"],
                "network": ["connection", "network", "dns", "ssl", "socket"],
                "server_error": ["server_error", "500", "502", "503", "504", "internal"]
            }
        }
        
        # æ·±åº¦åˆå¹¶é…ç½®
        for key, default_value in default_config.items():
            if key not in self.retry_config:
                self.retry_config[key] = default_value
            elif isinstance(default_value, dict) and isinstance(self.retry_config[key], dict):
                # å­—å…¸ç±»å‹çš„æ·±åº¦åˆå¹¶
                for sub_key, sub_default in default_value.items():
                    if sub_key not in self.retry_config[key]:
                        self.retry_config[key][sub_key] = sub_default

    def _should_retry(self, error: Exception, attempt: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•

        Args:
            error: é”™è¯¯å¯¹è±¡
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            æ˜¯å¦åº”è¯¥é‡è¯•
        """
        # è·å–é”™è¯¯ç±»å‹
        error_type = self._classify_error_type(error)
        
        # æ ¹æ®é”™è¯¯ç±»å‹è·å–æœ€å¤§é‡è¯•æ¬¡æ•°
        max_retries_for_error = self._get_max_retries_for_error_type(error_type)
        
        if attempt >= max_retries_for_error:
            return False

        # æ£€æŸ¥é”™è¯¯æ˜¯å¦å¯é‡è¯•
        return self._is_retryable_error_type(error_type)

    def _classify_error_type(self, error: Exception) -> str:
        """
        åˆ†ç±»é”™è¯¯ç±»å‹

        Args:
            error: é”™è¯¯å¯¹è±¡

        Returns:
            é”™è¯¯ç±»å‹å­—ç¬¦ä¸²
        """
        error_str = str(error).lower()
        
        # æ£€æŸ¥é…ç½®ä¸­çš„é”™è¯¯ç±»å‹æ¨¡å¼
        patterns = self.retry_config.get("retryable_error_patterns", {})
        
        for error_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                if pattern.lower() in error_str:
                    return error_type
        
        # é»˜è®¤é”™è¯¯ç±»å‹
        return "default"

    def _get_max_retries_for_error_type(self, error_type: str) -> int:
        """
        æ ¹æ®é”™è¯¯ç±»å‹è·å–æœ€å¤§é‡è¯•æ¬¡æ•°

        Args:
            error_type: é”™è¯¯ç±»å‹

        Returns:
            æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        max_retries_config = self.retry_config.get("max_retries_by_error_type", {})
        return max_retries_config.get(error_type, max_retries_config.get("default", self.max_retries))

    def _is_retryable_error_type(self, error_type: str) -> bool:
        """
        æ£€æŸ¥é”™è¯¯ç±»å‹æ˜¯å¦å¯é‡è¯•

        Args:
            error_type: é”™è¯¯ç±»å‹

        Returns:
            æ˜¯å¦å¯é‡è¯•
        """
        # æ‰€æœ‰åœ¨é…ç½®ä¸­çš„é”™è¯¯ç±»å‹éƒ½æ˜¯å¯é‡è¯•çš„
        max_retries_config = self.retry_config.get("max_retries_by_error_type", {})
        return error_type in max_retries_config and max_retries_config[error_type] > 0

    def _calculate_delay(self, attempt: int, error_type: str = "default") -> float:
        """
        è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰

        Args:
            attempt: å½“å‰å°è¯•æ¬¡æ•°
            error_type: é”™è¯¯ç±»å‹

        Returns:
            å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        import random

        # æ ¹æ®é”™è¯¯ç±»å‹è·å–åŸºç¡€å»¶è¿Ÿ
        base_delay_config = self.retry_config.get("base_delay_by_error_type", {})
        base_delay = base_delay_config.get(error_type, base_delay_config.get("default", self.base_delay))

        # æŒ‡æ•°é€€é¿
        delay = base_delay * (2 ** attempt)

        # åº”ç”¨æœ€å¤§å»¶è¿Ÿé™åˆ¶
        max_delay = self.retry_config.get("max_delay", 60.0)
        delay = min(delay, max_delay)

        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.retry_config.get("enable_jitter", True):
            jitter_range = self.retry_config.get("jitter_range", 0.25)
            jitter = delay * jitter_range * (random.random() * 2 - 1)
            delay += jitter

        return max(0.1, delay)

    def _update_retry_stats(self, error_type: str, success: bool) -> None:
        """
        æ›´æ–°é‡è¯•ç»Ÿè®¡ä¿¡æ¯

        Args:
            error_type: é”™è¯¯ç±»å‹
            success: é‡è¯•æ˜¯å¦æˆåŠŸ
        """
        if self.client and hasattr(self.client, 'stats'):
            retry_stats = self.client.stats.get("retry_stats", {})
            
            # æ›´æ–°æ€»é‡è¯•æ¬¡æ•°
            retry_stats["total_retries"] = retry_stats.get("total_retries", 0) + 1
            
            # æ›´æ–°æˆåŠŸ/å¤±è´¥é‡è¯•æ¬¡æ•°
            if success:
                retry_stats["successful_retries"] = retry_stats.get("successful_retries", 0) + 1
            else:
                retry_stats["failed_retries"] = retry_stats.get("failed_retries", 0) + 1
            
            # æ›´æ–°æŒ‰é”™è¯¯ç±»å‹çš„é‡è¯•æ¬¡æ•°
            retries_by_type = retry_stats.get("retries_by_error_type", {})
            retries_by_type[error_type] = retries_by_type.get(error_type, 0) + 1

class OpenAIClient:
    """OpenAI SDKå°è£…å®¢æˆ·ç«¯"""

    def __init__(self, config: Optional[SimpleOpenAIConfig] = None):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯

        Args:
            config: OpenAIé…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or SimpleOpenAIConfig()

        # è·å–æ—¥å¿—å™¨ï¼ˆä¼šè‡ªåŠ¨åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼‰
        self.logger = get_openai_logger()

        # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
        client_kwargs = self.config.to_openai_client_kwargs()
        self.async_client = AsyncOpenAI(**client_kwargs)

        # åˆ›å»ºé‡è¯•ç®¡ç†å™¨
        self.retry_manager = RetryManager(
            max_retries=self.config.max_retries,
            base_delay=self.config.retry_delay,
            retry_config=self.config.retry_config,
            client=self  # ä¼ é€’è‡ªèº«å¼•ç”¨ç”¨äºç»Ÿè®¡
        )

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            # æ–°å¢ï¼šè¯¦ç»†çš„é”™è¯¯ç»Ÿè®¡
            "error_stats": {
                "rate_limit": 0,
                "timeout": 0,
                "network": 0,
                "server_error": 0,
                "bad_request": 0,
                "authentication": 0,
                "permission": 0,
                "not_found": 0,
                "unknown": 0
            },
            # æ–°å¢ï¼šå»¶è¿Ÿç»Ÿè®¡
            "latency_stats": {
                "min": float('inf'),
                "max": 0.0,
                "avg": 0.0,
                "total_count": 0,
                "sum": 0.0
            },
            # æ–°å¢ï¼šé‡è¯•ç»Ÿè®¡
            "retry_stats": {
                "total_retries": 0,
                "successful_retries": 0,
                "failed_retries": 0,
                "retries_by_error_type": {
                    "rate_limit": 0,
                    "timeout": 0,
                    "network": 0,
                    "server_error": 0,
                    "default": 0
                }
            }
        }


        # è®°å½•åˆå§‹åŒ–æ—¥å¿—
        self.logger.info(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.config.model}, åŸºç¡€URL: {self.config.base_url}")

    def _prepare_messages(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None
    ) -> List[Message]:
        """
        å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼Œç»Ÿä¸€å¤„ç†ç³»ç»Ÿæç¤ºè¯å’Œå…¨å±€ç³»ç»Ÿæç¤ºè¯

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            å‡†å¤‡å¥½çš„æ¶ˆæ¯åˆ—è¡¨
        """
        prepared_messages = []

        # æ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            prepared_messages.append({"role": "system", "content": system_prompt})

        # æ·»åŠ å¯¹è¯æ¶ˆæ¯
        if messages:
            prepared_messages.extend(messages)

        return prepared_messages

    def _prepare_request_params(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        stream: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‡†å¤‡APIè¯·æ±‚å‚æ•°

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            stream: æ˜¯å¦æµå¼å“åº”
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å‡†å¤‡å¥½çš„è¯·æ±‚å‚æ•°
        """
        # å‡†å¤‡æ¶ˆæ¯
        prepared_messages = self._prepare_messages(messages, system_prompt)

        # åˆå¹¶é…ç½®å‚æ•°
        params = self.config.to_chat_completion_kwargs()

        # è¿‡æ»¤æ‰å†…éƒ¨å‚æ•°ï¼Œé¿å…ä¼ é€’ç»™OpenAI API
        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ['filter_tool_tags']}
        params.update(filtered_kwargs)
        params["messages"] = prepared_messages

        if stream:
            params["stream"] = True

        # æ·»åŠ å·¥å…·æ”¯æŒ
        if tools and self.config.function_calling_enabled:
            params["tools"] = tools
            if "tool_choice" not in params:
                params["tool_choice"] = self.config.tool_choice

        return params

    def _update_success_stats(self, response: Any, latency: Optional[float] = None) -> None:
        """æ›´æ–°æˆåŠŸç»Ÿè®¡ä¿¡æ¯"""
        self.stats["successful_requests"] += 1
        if hasattr(response, 'usage') and response.usage:
            self.stats["total_tokens"] += response.usage.total_tokens
        
        # æ›´æ–°å»¶è¿Ÿç»Ÿè®¡
        if latency is not None:
            self._update_latency_stats(latency)

    def _update_latency_stats(self, latency: float) -> None:
        """æ›´æ–°å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats["latency_stats"]
        stats["min"] = min(stats["min"], latency)
        stats["max"] = max(stats["max"], latency)
        stats["total_count"] += 1
        stats["sum"] += latency
        stats["avg"] = stats["sum"] / stats["total_count"]

    def _update_failure_stats(self, error: Optional[OpenAIClientError] = None) -> None:
        """æ›´æ–°å¤±è´¥ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["failed_requests"] += 1
        
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ç±»å‹ç»Ÿè®¡
        if error and hasattr(error, 'error_type'):
            error_type = error.error_type
            if error_type in self.stats["error_stats"]:
                self.stats["error_stats"][error_type] += 1
            else:
                self.stats["error_stats"]["unknown"] += 1



    async def chat_completion(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> ChatCompletion:
        """
        å¼‚æ­¥èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            èŠå¤©å®Œæˆå“åº”
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion", params)

            # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡ŒAPIè°ƒç”¨
            async def _api_call():
                return await self.async_client.chat.completions.create(**params)

            response = await self.retry_manager.execute_with_retry(_api_call)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«å»¶è¿Ÿä¿¡æ¯ï¼‰
            request_latency = time.time() - start_time
            self._update_success_stats(response, request_latency)

            # è®°å½•å“åº”æ—¥å¿—
            self._log_response("chat_completion", response)

            return response

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time
    
    async def chat_completion_stream(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        filter_tool_tags: bool = False,
        **kwargs
    ) -> AsyncIterator[ChatCompletionChunk]:
        """
        å¼‚æ­¥æµå¼èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            filter_tool_tags: æ˜¯å¦è¿‡æ»¤å·¥å…·è°ƒç”¨æ ‡ç­¾ï¼ˆé»˜è®¤Falseï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            èŠå¤©å®Œæˆæµå¼å“åº”å—ï¼ˆå¦‚æœå¯ç”¨filter_tool_tagsï¼Œdelta.contentå°†è¢«è¿‡æ»¤ï¼‰
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # æå–filter_tool_tagså‚æ•°ï¼Œé¿å…ä¼ é€’ç»™OpenAI API
            filter_tool_tags_param = filter_tool_tags

        

            # å‡†å¤‡è¯·æ±‚å‚æ•°ï¼ˆä¸åŒ…å«filter_tool_tagsï¼‰
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                stream=True,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion_stream", params)

            # æ‰§è¡Œæµå¼APIè°ƒç”¨
            stream = await self.async_client.chat.completions.create(**params)

            chunk_count = 0
            full_content = ""
            total_tokens = 0

            # åˆå§‹åŒ–å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            tag_filter = ToolCallTagFilter() if filter_tool_tags_param else None

            async for chunk in stream:
                chunk_count += 1

                # æ”¶é›†å“åº”å†…å®¹ç”¨äºæ—¥å¿—è®°å½•
                if chunk.choices and chunk.choices[0].delta.content:
                    full_content += chunk.choices[0].delta.content

                # æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
                if hasattr(chunk, 'usage') and chunk.usage:
                    total_tokens = chunk.usage.total_tokens



                # å¦‚æœå¯ç”¨äº†å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤ï¼Œå¤„ç†delta.content
                if filter_tool_tags_param and tag_filter and chunk.choices and chunk.choices[0].delta.content:
                    # åˆ›å»ºchunkçš„å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹å¯¹è±¡
                    filtered_chunk = copy.deepcopy(chunk)
                    original_content = chunk.choices[0].delta.content
                    filtered_content = tag_filter.process_chunk(original_content)

                    # æ›´æ–°å‰¯æœ¬çš„content
                    filtered_chunk.choices[0].delta.content = filtered_content

                    # å¦‚æœæå–åˆ°äº†å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°delta.tool_calls
                    extracted_calls = tag_filter.get_extracted_tool_calls()
                    if extracted_calls and not filtered_chunk.choices[0].delta.tool_calls:
                        # å°†æå–çš„å·¥å…·è°ƒç”¨è½¬æ¢ä¸ºdeltaæ ¼å¼
                        tool_call_deltas = []
                        for i, tool_call in enumerate(extracted_calls):
                            from openai.types.chat.chat_completion_chunk import ChoiceDeltaToolCall, ChoiceDeltaToolCallFunction

                            tool_call_delta = ChoiceDeltaToolCall(
                                index=i,
                                id=tool_call["id"],
                                function=ChoiceDeltaToolCallFunction(
                                    name=tool_call["function"]["name"],
                                    arguments=tool_call["function"]["arguments"]
                                ),
                                type="function"
                            )
                            tool_call_deltas.append(tool_call_delta)

                        filtered_chunk.choices[0].delta.tool_calls = tool_call_deltas
                        # æ¸…ç©ºå·²å¤„ç†çš„å·¥å…·è°ƒç”¨ï¼Œé¿å…é‡å¤
                        tag_filter.extracted_tool_calls.clear()

                    yield filtered_chunk
                else:
                    yield chunk

            # å¦‚æœå¯ç”¨äº†è¿‡æ»¤ï¼Œå¤„ç†å‰©ä½™çš„å†…å®¹
            if filter_tool_tags_param and tag_filter:
                remaining_content = tag_filter.finalize()
                if remaining_content:
                    # åˆ›å»ºä¸€ä¸ªåŒ…å«å‰©ä½™å†…å®¹çš„chunk
                    from openai.types.chat.chat_completion_chunk import ChatCompletionChunk, Choice, ChoiceDelta

                    # åˆ›å»ºæœ€åä¸€ä¸ªchunkæ¥è¾“å‡ºå‰©ä½™å†…å®¹
                    final_choice = Choice(
                        delta=ChoiceDelta(content=remaining_content),
                        index=0,
                        finish_reason=None
                    )
                    final_chunk = ChatCompletionChunk(
                        id="filtered_final",
                        choices=[final_choice],
                        created=int(time.time()),
                        model="filtered",
                        object="chat.completion.chunk"
                    )
                    yield final_chunk

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«å»¶è¿Ÿä¿¡æ¯ï¼‰
            self.stats["successful_requests"] += 1
            if total_tokens > 0:
                self.stats["total_tokens"] += total_tokens
            
            # æ›´æ–°å»¶è¿Ÿç»Ÿè®¡ï¼ˆæµå¼è¯·æ±‚çš„å»¶è¿Ÿè®¡ç®—ï¼‰
            request_latency = time.time() - start_time
            self._update_latency_stats(request_latency)

            # è®°å½•å“åº”æ—¥å¿—ï¼ˆæµå¼å“åº”ï¼‰
            self._log_stream_response("chat_completion_stream", chunk_count, full_content)

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time




    
    def _handle_error(self, error: Exception, request_params: Optional[Dict[str, Any]] = None) -> OpenAIClientError:
        """
        å¤„ç†å’Œè½¬æ¢é”™è¯¯ï¼Œæä¾›è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯

        Args:
            error: åŸå§‹é”™è¯¯
            request_params: è¯·æ±‚å‚æ•°ï¼ˆç”¨äºè¯Šæ–­ï¼‰

        Returns:
            è½¬æ¢åçš„é”™è¯¯ï¼ŒåŒ…å«è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
        """
        import openai

        # å‡†å¤‡è¯·æ±‚ä¿¡æ¯ï¼ˆå»é™¤æ•æ„Ÿä¿¡æ¯ï¼‰
        safe_request_info = self._prepare_safe_request_info(request_params)
        
        # OpenAI SDKç‰¹å®šé”™è¯¯
        if isinstance(error, openai.RateLimitError):
            retry_after = getattr(error, 'retry_after', None)
            return OpenAIRateLimitError(
                f"API rate limit exceeded: {error}",
                original_error=error,
                request_info=safe_request_info,
                retry_after=retry_after
            )

        if isinstance(error, openai.APITimeoutError):
            return OpenAITimeoutError(
                f"API request timeout: {error}",
                original_error=error,
                request_info=safe_request_info
            )

        if isinstance(error, openai.APIConnectionError):
            return OpenAIRetryableError(
                f"API connection error: {error}",
                original_error=error,
                request_info=safe_request_info,
                error_type="network"
            )

        if isinstance(error, openai.InternalServerError):
            return OpenAIRetryableError(
                f"Internal server error: {error}",
                original_error=error,
                request_info=safe_request_info,
                error_type="server_error"
            )

        if isinstance(error, openai.BadRequestError):
            return OpenAIClientError(
                f"Bad request: {error}",
                original_error=error,
                request_info=safe_request_info,
                error_type="bad_request"
            )

        if isinstance(error, openai.AuthenticationError):
            return OpenAIClientError(
                f"Authentication failed: {error}",
                original_error=error,
                request_info=safe_request_info,
                error_type="authentication"
            )

        if isinstance(error, openai.PermissionDeniedError):
            return OpenAIClientError(
                f"Permission denied: {error}",
                original_error=error,
                request_info=safe_request_info,
                error_type="permission"
            )

        if isinstance(error, openai.NotFoundError):
            return OpenAIClientError(
                f"Resource not found: {error}",
                original_error=error,
                request_info=safe_request_info,
                error_type="not_found"
            )

        # é€šç”¨é”™è¯¯å¤„ç†
        error_message = str(error)

        # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "rate_limit" in error_message.lower() or "429" in error_message:
            return OpenAIRateLimitError(
                f"API rate limit exceeded: {error_message}",
                original_error=error,
                request_info=safe_request_info
            )

        # è¶…æ—¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "timeout" in error_message.lower() or "timed out" in error_message.lower():
            return OpenAITimeoutError(
                f"API request timeout: {error_message}",
                original_error=error,
                request_info=safe_request_info
            )

        # ç½‘ç»œé”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(keyword in error_message.lower() for keyword in ["connection", "network", "dns", "ssl", "socket"]):
            return OpenAIRetryableError(
                f"Network error: {error_message}",
                original_error=error,
                request_info=safe_request_info,
                error_type="network"
            )

        # æœåŠ¡å™¨é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(code in error_message for code in ["500", "502", "503", "504"]):
            return OpenAIRetryableError(
                f"Server error: {error_message}",
                original_error=error,
                request_info=safe_request_info,
                error_type="server_error"
            )

        # å…¶ä»–é”™è¯¯
        return OpenAIClientError(
            f"OpenAI API error: {error_message}",
            original_error=error,
            request_info=safe_request_info,
            error_type="unknown"
        )

    def _get_user_friendly_error_message(self, error: OpenAIClientError) -> str:
        """
        ç”Ÿæˆç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯

        Args:
            error: OpenAIClientErrorå®ä¾‹

        Returns:
            ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
        """
        error_type = getattr(error, 'error_type', 'unknown')
        
        friendly_messages = {
            "rate_limit": "âš ï¸ APIè®¿é—®é¢‘ç‡å—é™ï¼Œè¯·ç¨åå†è¯•ã€‚å¦‚æœæ‚¨é¢‘ç¹é‡åˆ°æ­¤é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘å‡çº§APIå¥—é¤æˆ–è”ç³»æ”¯æŒã€‚",
            "timeout": "â±ï¸ è¯·æ±‚è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œè¿æ¥è¾ƒæ…¢æˆ–æœåŠ¡å™¨å“åº”å»¶è¿Ÿã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•ã€‚",
            "network": "ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼Œç„¶åé‡è¯•ã€‚",
            "server_error": "ğŸ”§ æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨ï¼Œå¯èƒ½æ˜¯OpenAIæœåŠ¡ç»´æŠ¤ä¸­ã€‚è¯·ç¨åé‡è¯•ã€‚",
            "bad_request": "âŒ è¯·æ±‚æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚",
            "authentication": "ğŸ”‘ APIå¯†é’¥éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®ã€‚",
            "permission": "ğŸš« æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥æ˜¯å¦æœ‰è®¿é—®è¯¥èµ„æºçš„æƒé™ã€‚",
            "not_found": "ğŸ” è¯·æ±‚çš„èµ„æºä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦æ­£ç¡®ã€‚",
            "default": "âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
        }
        
        # è·å–å…·ä½“çš„é”™è¯¯æ¶ˆæ¯
        base_message = friendly_messages.get(error_type, friendly_messages["default"])
        
        # æ·»åŠ é‡è¯•å»ºè®®ï¼ˆå¦‚æœæ˜¯å¯é‡è¯•é”™è¯¯ï¼‰
        if error_type in ["rate_limit", "timeout", "network", "server_error"]:
            if hasattr(error, 'retry_after') and error.retry_after:
                base_message += f" å»ºè®®ç­‰å¾… {error.retry_after:.0f} ç§’åé‡è¯•ã€‚"
            else:
                base_message += " ç³»ç»Ÿå°†è‡ªåŠ¨é‡è¯•ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»ã€‚"
        
        return base_message

    def _create_fallback_response(self, error: Exception) -> Dict[str, Any]:
        """
        åˆ›å»ºä¼˜é›…é™çº§çš„å“åº”

        Args:
            error: å‘ç”Ÿçš„é”™è¯¯

        Returns:
            é™çº§å“åº”å­—å…¸
        """
        error_type = "unknown"
        if isinstance(error, OpenAIClientError):
            error_type = getattr(error, 'error_type', 'unknown')
        
        # æ ¹æ®é”™è¯¯ç±»å‹æä¾›ä¸åŒçš„é™çº§å“åº”
        fallback_responses = {
            "rate_limit": {
                "error": "rate_limit_exceeded",
                "message": "APIè®¿é—®é¢‘ç‡å—é™ï¼Œè¯·ç¨åå†è¯•",
                "suggestion": "ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•æˆ–å‡çº§APIå¥—é¤",
                "retryable": True
            },
            "timeout": {
                "error": "request_timeout", 
                "message": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥",
                "suggestion": "æ£€æŸ¥ç½‘ç»œç¨³å®šæ€§åé‡è¯•",
                "retryable": True
            },
            "network": {
                "error": "network_error",
                "message": "ç½‘ç»œè¿æ¥é—®é¢˜",
                "suggestion": "æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•",
                "retryable": True
            },
            "default": {
                "error": "service_unavailable",
                "message": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
                "suggestion": "è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ",
                "retryable": False
            }
        }
        
        return fallback_responses.get(error_type, fallback_responses["default"])

    def _prepare_safe_request_info(self, request_params: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å‡†å¤‡å®‰å…¨çš„è¯·æ±‚ä¿¡æ¯ï¼ˆå»é™¤æ•æ„Ÿæ•°æ®ï¼‰

        Args:
            request_params: åŸå§‹è¯·æ±‚å‚æ•°

        Returns:
            å®‰å…¨çš„è¯·æ±‚ä¿¡æ¯å­—å…¸
        """
        if not request_params:
            return {}
        
        # å¤åˆ¶å‚æ•°å¹¶ç§»é™¤æ•æ„Ÿä¿¡æ¯
        safe_params = request_params.copy()
        
        # ç§»é™¤æ•æ„Ÿå­—æ®µ
        sensitive_fields = ["api_key", "password", "token", "secret", "key"]
        for field in sensitive_fields:
            if field in safe_params:
                safe_params[field] = "[REDACTED]"
        
        # å¤„ç†æ¶ˆæ¯ä¸­çš„æ•æ„Ÿå†…å®¹
        if "messages" in safe_params and isinstance(safe_params["messages"], list):
            for message in safe_params["messages"]:
                if isinstance(message, dict) and "content" in message:
                    # ç®€ç•¥æ˜¾ç¤ºæ¶ˆæ¯å†…å®¹
                    content = message["content"]
                    if isinstance(content, str) and len(content) > 100:
                        message["content"] = content[:100] + "..."
        
        return safe_params
    
    def _log_request(self, method: str, params: Dict[str, Any]) -> None:
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        if self.config.log_requests:
            self.logger.info(f"ğŸ”„ OpenAI {method} è¯·æ±‚:")
            self.logger.info(f"ï¿½ å®Œæ•´è¯·æ±‚å‚æ•°: {params}")

    def _log_response(self, method: str, response: Any) -> None:
        """è®°å½•å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} å“åº”:")
            self.logger.info(f"ğŸ“ å®Œæ•´å“åº”: {response}")

    def _log_stream_response(self, method: str, chunk_count: int, content: str = "") -> None:
        """è®°å½•æµå¼å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} æµå¼å“åº”å®Œæˆ: æ¥æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—")
            if content:
                self.logger.info(f"ğŸ“ å®Œæ•´æµå¼å“åº”å†…å®¹: {content}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def reset_stats(self) -> None:
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "error_stats": {
                "rate_limit": 0,
                "timeout": 0,
                "network": 0,
                "server_error": 0,
                "bad_request": 0,
                "authentication": 0,
                "permission": 0,
                "not_found": 0,
                "unknown": 0
            },
            "latency_stats": {
                "min": float('inf'),
                "max": 0.0,
                "avg": 0.0,
                "total_count": 0,
                "sum": 0.0
            },
            "retry_stats": {
                "total_retries": 0,
                "successful_retries": 0,
                "failed_retries": 0,
                "retries_by_error_type": {
                    "rate_limit": 0,
                    "timeout": 0,
                    "network": 0,
                    "server_error": 0,
                    "default": 0
                }
            }
        }

    def get_user_friendly_error(self, error: Exception) -> str:
        """
        è·å–ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯ï¼ˆå…¬å…±æ–¹æ³•ï¼‰

        Args:
            error: å¼‚å¸¸å¯¹è±¡

        Returns:
            ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
        """
        if isinstance(error, OpenAIClientError):
            return self._get_user_friendly_error_message(error)
        else:
            # å¤„ç†éOpenAIClientErrorå¼‚å¸¸
            handled_error = self._handle_error(error)
            return self._get_user_friendly_error_message(handled_error)


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_global_client: Optional[OpenAIClient] = None


def get_openai_client(config: Optional[SimpleOpenAIConfig] = None) -> OpenAIClient:
    """
    è·å–å…¨å±€OpenAIå®¢æˆ·ç«¯å®ä¾‹

    Args:
        config: OpenAIé…ç½®å¯¹è±¡

    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
    """
    global _global_client

    if _global_client is None or config is not None:
        _global_client = OpenAIClient(config)

    return _global_client

