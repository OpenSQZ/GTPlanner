"""
é¢„åˆ¶ä»¶æ¨èèŠ‚ç‚¹ (Node_Prefab_Recommend)

åŸºäºå‘é‡æœåŠ¡è¿›è¡Œé¢„åˆ¶ä»¶æ¨èã€‚
å¦‚æœå‘é‡æœåŠ¡ä¸å¯ç”¨ï¼ŒèŠ‚ç‚¹ä¼šè¿”å›é”™è¯¯ï¼Œagent å¯ä»¥é€‰æ‹©ä½¿ç”¨ search_prefabs å·¥å…·ä½œä¸ºé™çº§æ–¹æ¡ˆã€‚
"""

import time
import requests
import asyncio
import json
from typing import Dict, List, Any, Optional
from pocketflow import AsyncNode
from gtplanner.utils.openai_client import OpenAIClient
from gtplanner.utils.config_manager import get_vector_service_config
from gtplanner.agent.streaming import (
    emit_processing_status,
    emit_error
)

# å¯¼å…¥å¤šè¯­è¨€æç¤ºè¯ç³»ç»Ÿ
from gtplanner.agent.prompts import get_prompt, PromptTypes


class NodePrefabRecommend(AsyncNode):
    """é¢„åˆ¶ä»¶æ¨èèŠ‚ç‚¹ï¼ˆåŸºäºå‘é‡æœåŠ¡ï¼‰"""
    
    def __init__(self, max_retries: int = 3, wait: float = 2.0):
        """
        åˆå§‹åŒ–é¢„åˆ¶ä»¶æ¨èèŠ‚ç‚¹
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait: é‡è¯•ç­‰å¾…æ—¶é—´
        """
        super().__init__(max_retries=max_retries, wait=wait)
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½å‘é‡æœåŠ¡é…ç½®
        vector_config = get_vector_service_config()
        self.vector_service_url = vector_config.get("base_url")
        self.timeout = vector_config.get("timeout", 30)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–ç´¢å¼•ç›¸å…³å‚æ•°
        self.index_name = vector_config.get("prefabs_index_name", "document_gtplanner_prefabs")
        self.vector_field = vector_config.get("vector_field", "combined_text")
        
        # æ¨èé…ç½®
        self.default_top_k = 5
        self.min_score_threshold = 0.4  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆæé«˜åˆ°0.4ä»¥è¿‡æ»¤ä¸ç›¸å…³ç»“æœï¼‰
        self.use_llm_filter = True  # æ˜¯å¦ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰
        self.llm_candidate_count = 10  # ä¼ ç»™å¤§æ¨¡å‹çš„å€™é€‰æ•°é‡
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = OpenAIClient()
        
        # æ£€æŸ¥å‘é‡æœåŠ¡å¯ç”¨æ€§
        self.vector_service_available = self._check_vector_service()
        
        if not self.vector_service_available:
            print("âš ï¸ å‘é‡æœåŠ¡ä¸å¯ç”¨ï¼Œprefab_recommend èŠ‚ç‚¹å°†æ— æ³•å·¥ä½œ")
            print("ğŸ’¡ æç¤ºï¼šå¯ä»¥ä½¿ç”¨ search_prefabs å·¥å…·ä½œä¸ºé™çº§æ–¹æ¡ˆ")
    
    def _check_vector_service(self) -> bool:
        """æ£€æŸ¥å‘é‡æœåŠ¡æ˜¯å¦å¯ç”¨"""
        if not self.vector_service_url:
            return False
        try:
            response = requests.get(f"{self.vector_service_url}/health", timeout=5)
            return response.status_code == 200
        except Exception:
            return False
    
    async def prep_async(self, shared) -> Dict[str, Any]:
        """
        å‡†å¤‡é˜¶æ®µï¼šä»å…±äº«å˜é‡è·å–æŸ¥è¯¢å‚æ•°
        
        Args:
            shared: pocketflowå­—å…¸å…±äº«å˜é‡
            
        Returns:
            å‡†å¤‡ç»“æœå­—å…¸
        """
        try:
            # ä»å…±äº«å˜é‡è·å–æŸ¥è¯¢å‚æ•°
            query = shared.get("query", "")
            top_k = shared.get("top_k", self.default_top_k)
            index_name = shared.get("index_name", self.index_name)
            min_score = shared.get("min_score", self.min_score_threshold)
            use_llm_filter = shared.get("use_llm_filter", self.use_llm_filter)
            language = shared.get("language")
            
            # å¦‚æœæ²¡æœ‰æä¾›æŸ¥è¯¢ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæå–
            if not query:
                query = self._extract_query_from_shared_state(shared)
            
            # éªŒè¯è¾“å…¥
            if not query or not query.strip():
                return {
                    "error": "No query provided for prefab recommendation",
                    "query": "",
                    "top_k": top_k,
                    "index_name": index_name
                }
            
            # é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬
            processed_query = query.strip()
            
            return {
                "query": processed_query,
                "original_query": query,
                "top_k": top_k,
                "index_name": index_name,
                "min_score": min_score,
                "use_llm_filter": use_llm_filter,
                "language": language,
                "streaming_session": shared.get("streaming_session")
            }
            
        except Exception as e:
            return {
                "error": f"Prefab recommendation preparation failed: {str(e)}",
                "query": "",
                "top_k": self.default_top_k,
                "index_name": self.index_name
            }
    
    async def exec_async(self, prep_res: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œé˜¶æ®µï¼šè°ƒç”¨å‘é‡æœåŠ¡è¿›è¡Œé¢„åˆ¶ä»¶æ£€ç´¢
        
        Args:
            prep_res: å‡†å¤‡é˜¶æ®µçš„ç»“æœ
            
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        if "error" in prep_res:
            raise ValueError(prep_res["error"])
        
        query = prep_res["query"]
        top_k = prep_res["top_k"]
        index_name = prep_res["index_name"]
        min_score = prep_res["min_score"]
        use_llm_filter = prep_res["use_llm_filter"]
        language = prep_res["language"]
        
        if not query:
            raise ValueError("Empty query for prefab recommendation")
        
        if not self.vector_service_available:
            raise RuntimeError(
                "Vector service is not available. "
                "Please use 'search_prefabs' tool as a fallback."
            )
        
        try:
            start_time = time.time()
            
            # è°ƒç”¨å‘é‡æœåŠ¡è¿›è¡Œæ£€ç´¢ï¼ˆè·å–æ›´å¤šå€™é€‰ï¼‰
            search_top_k = max(top_k, self.llm_candidate_count) if use_llm_filter else top_k
            shared_for_events = {"streaming_session": prep_res.get("streaming_session")}
            search_results = await self._search_prefabs_vector(query, index_name, search_top_k, shared_for_events)
            
            # è¿‡æ»¤å’Œå¤„ç†ç»“æœ
            filtered_results = self._filter_results(search_results, min_score=min_score)
            processed_results = self._process_results(filtered_results)
            
            # ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_llm_filter and len(processed_results) > 1:
                try:
                    llm_selected_results = await self._llm_filter_prefabs(
                        query, processed_results, top_k, language, shared_for_events
                    )
                    processed_results = llm_selected_results
                    await emit_processing_status(
                        shared_for_events, 
                        f"âœ… å¤§æ¨¡å‹ç­›é€‰å®Œæˆï¼Œè¿”å› {len(processed_results)} ä¸ªé¢„åˆ¶ä»¶"
                    )
                except Exception as e:
                    await emit_error(
                        shared_for_events, 
                        f"âš ï¸ å¤§æ¨¡å‹ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’åº: {str(e)}"
                    )
                    processed_results = processed_results[:top_k]
            else:
                processed_results = processed_results[:top_k]
            
            search_time = time.time() - start_time
            
            return {
                "recommended_prefabs": processed_results,
                "total_found": len(processed_results),
                "search_time": round(search_time * 1000),  # è½¬æ¢ä¸ºæ¯«ç§’
                "query_used": query,
                "original_query": prep_res["original_query"],
                "search_mode": "vector_rag",
                "search_metadata": {
                    "index_name": index_name,
                    "top_k": top_k,
                    "min_score": min_score
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"Prefab recommendation execution failed: {str(e)}")
    
    async def post_async(self, shared, prep_res: Dict[str, Any], exec_res: Dict[str, Any]) -> str:
        """
        åå¤„ç†é˜¶æ®µï¼šå°†æ¨èç»“æœå­˜å‚¨åˆ°å…±äº«çŠ¶æ€
        
        Args:
            shared: å…±äº«çŠ¶æ€å¯¹è±¡
            prep_res: å‡†å¤‡é˜¶æ®µç»“æœ
            exec_res: æ‰§è¡Œé˜¶æ®µç»“æœ
            
        Returns:
            ä¸‹ä¸€æ­¥åŠ¨ä½œ
        """
        try:
            if "error" in exec_res:
                if hasattr(shared, 'record_error'):
                    shared.record_error(Exception(exec_res["error"]), "NodePrefabRecommend.exec")
                return "error"
            
            recommended_prefabs = exec_res["recommended_prefabs"]
            total_found = exec_res["total_found"]
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å­æµç¨‹çš„å…±äº«å˜é‡ï¼ˆå­—å…¸ç±»å‹ï¼‰
            if isinstance(shared, dict):
                # å­æµç¨‹æ¨¡å¼ï¼šä¿å­˜æ¨èç»“æœåˆ°å…±äº«å˜é‡
                shared["recommended_prefabs"] = recommended_prefabs
                shared["prefab_recommendation_result"] = {
                    "prefabs": recommended_prefabs,
                    "total_found": total_found,
                    "query": exec_res["query_used"],
                    "search_time": exec_res["search_time"],
                    "search_mode": exec_res["search_mode"]
                }
                return "success"
            
            # ä¸»æµç¨‹æ¨¡å¼ï¼šä¿å­˜åˆ°ç ”ç©¶å‘ç°æˆ–ç›¸åº”çš„çŠ¶æ€
            if not hasattr(shared, 'prefab_recommendations'):
                shared.prefab_recommendations = []
            
            # è½¬æ¢æ¨èç»“æœä¸ºæ ‡å‡†æ ¼å¼
            for prefab in recommended_prefabs:
                recommendation = {
                    "prefab_id": prefab["id"],
                    "prefab_name": prefab.get("summary", ""),
                    "description": prefab.get("description", ""),
                    "relevance_score": prefab.get("score", 0.0),
                    "tags": prefab.get("tags", ""),
                    "metadata": {
                        "version": prefab.get("version", ""),
                        "author": prefab.get("author", ""),
                        "repo_url": prefab.get("repo_url", ""),
                        "artifact_url": prefab.get("artifact_url", "")
                    },
                    "search_metadata": {
                        "query": exec_res["query_used"],
                        "search_time": exec_res["search_time"],
                        "search_mode": exec_res["search_mode"]
                    }
                }
                shared.prefab_recommendations.append(recommendation)
            
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯è®°å½•æ¨èç»“æœ
            if hasattr(shared, 'add_system_message'):
                shared.add_system_message(
                    f"é¢„åˆ¶ä»¶æ¨èå®Œæˆï¼Œæ‰¾åˆ° {total_found} ä¸ªç›¸å…³é¢„åˆ¶ä»¶",
                    agent_source="NodePrefabRecommend",
                    query=exec_res["query_used"],
                    prefabs_count=total_found,
                    search_time_ms=exec_res["search_time"]
                )
            
            return "success"
            
        except Exception as e:
            if hasattr(shared, 'record_error'):
                shared.record_error(e, "NodePrefabRecommend.post")
            return "error"
    
    def exec_fallback(self, prep_res: Dict[str, Any], exc: Exception) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¤±è´¥æ—¶çš„é™çº§å¤„ç†
        
        Args:
            prep_res: å‡†å¤‡é˜¶æ®µç»“æœ
            exc: å¼‚å¸¸å¯¹è±¡
            
        Returns:
            é”™è¯¯ä¿¡æ¯
        """
        return {
            "error": f"Prefab recommendation execution failed: {str(exc)}",
            "recommended_prefabs": [],
            "total_found": 0,
            "query_used": prep_res.get("query", ""),
            "original_query": prep_res.get("original_query", ""),
            "search_mode": "failed"
        }
    
    def _extract_query_from_shared_state(self, shared) -> str:
        """ä»å…±äº«çŠ¶æ€ä¸­æå–æŸ¥è¯¢æ–‡æœ¬"""
        query_candidates = []
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å­æµç¨‹çš„å…±äº«å˜é‡ï¼ˆå­—å…¸ç±»å‹ï¼‰
        if isinstance(shared, dict):
            if "user_query" in shared:
                query_candidates.append(shared["user_query"])
            if "search_query" in shared:
                query_candidates.append(shared["search_query"])
            if "task_description" in shared:
                query_candidates.append(shared["task_description"])
        else:
            # ä¸»æµç¨‹æ¨¡å¼
            if hasattr(shared, 'user_intent') and hasattr(shared.user_intent, 'original_query'):
                query_candidates.append(shared.user_intent.original_query)
            if hasattr(shared, 'user_requirements') and shared.user_requirements:
                query_candidates.append(shared.user_requirements)
            if hasattr(shared, 'short_planning') and shared.short_planning:
                query_candidates.append(shared.short_planning)
        
        # è¿”å›ç¬¬ä¸€ä¸ªéç©ºçš„æŸ¥è¯¢
        for candidate in query_candidates:
            if candidate and candidate.strip():
                return candidate.strip()
        
        return ""
    
    async def _search_prefabs_vector(
        self, 
        query: str, 
        index_name: str, 
        top_k: int,
        shared: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è°ƒç”¨å‘é‡æœåŠ¡è¿›è¡Œé¢„åˆ¶ä»¶æ£€ç´¢"""
        try:
            # æ„å»ºæœç´¢è¯·æ±‚
            search_request = {
                "query": query,
                "vector_field": self.vector_field,
                "index": index_name,
                "top_k": top_k
            }
            
            # è°ƒç”¨å‘é‡æœåŠ¡
            response = requests.post(
                f"{self.vector_service_url}/search",
                json=search_request,
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                result = response.json()
                total_results = result.get('total', 0)
                
                # æ‰“å°æ¯ä¸ªé¢„åˆ¶ä»¶çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
                if result.get('results'):
                    print(f"\nğŸ” å‘é‡æ£€ç´¢ç»“æœ (æŸ¥è¯¢: '{query}'):")
                    for idx, item in enumerate(result['results'][:10], 1):
                        doc = item.get('document', {})
                        score = item.get('score', 0)
                        name = doc.get('name', 'Unknown')
                        print(f"  {idx}. [{score:.3f}] {name}")
                
                await emit_processing_status(
                    shared, 
                    f"âœ… æ£€ç´¢åˆ° {total_results} ä¸ªç›¸å…³é¢„åˆ¶ä»¶"
                )
                return result
            else:
                error_msg = f"å‘é‡æœåŠ¡è¿”å›é”™è¯¯: {response.status_code}, {response.text}"
                await emit_error(shared, f"âŒ {error_msg}")
                raise RuntimeError(error_msg)
                
        except requests.exceptions.RequestException as e:
            error_msg = f"è°ƒç”¨å‘é‡æœåŠ¡å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            raise RuntimeError(error_msg)
    
    def _filter_results(
        self, 
        search_results: Dict[str, Any],
        min_score: float = 0.0
    ) -> List[Dict[str, Any]]:
        """è¿‡æ»¤æœç´¢ç»“æœ"""
        if "results" not in search_results:
            return []
        
        filtered = []
        filtered_out_count = 0
        for result in search_results["results"]:
            # æ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
            score = result.get("score", 0.0)
            if score < min_score:
                filtered_out_count += 1
                continue
            
            # æ·»åŠ åˆ†æ•°åˆ°æ–‡æ¡£ä¸­
            document = result.get("document", {})
            document["score"] = score
            filtered.append(document)
        
        if filtered_out_count > 0:
            print(f"ğŸ“Š ç›¸ä¼¼åº¦è¿‡æ»¤: ä¿ç•™ {len(filtered)} ä¸ª (è¿‡æ»¤æ‰ {filtered_out_count} ä¸ªï¼Œé˜ˆå€¼: {min_score:.2f})")
        
        return filtered
    
    def _process_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åå¤„ç†æœç´¢ç»“æœ"""
        processed = []
        
        for result in results:
            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            processed_result = {
                "id": result.get("id", ""),
                "type": result.get("type", "PREFAB"),
                "summary": result.get("summary", ""),
                "description": result.get("description", ""),
                "tags": result.get("tags", ""),
                "score": result.get("score", 0.0),
                # é¢„åˆ¶ä»¶ç‰¹æœ‰å­—æ®µ
                "version": result.get("version", ""),
                "author": result.get("author", ""),
                "repo_url": result.get("repo_url", ""),
                "artifact_url": result.get("artifact_url", ""),
                "created_at": result.get("created_at", ""),
                "updated_at": result.get("updated_at", "")
            }
            
            processed.append(processed_result)
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
        processed.sort(key=lambda x: x["score"], reverse=True)
        
        return processed
    
    async def _llm_filter_prefabs(
        self, 
        query: str, 
        prefabs: List[Dict[str, Any]], 
        top_k: int,
        language: str,
        shared: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰æœ€åˆé€‚çš„é¢„åˆ¶ä»¶"""
        if not prefabs:
            return []
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_filter_prompt(query, prefabs, top_k, language)
        
        try:
            # è°ƒç”¨å¤§æ¨¡å‹
            messages = [{"role": "user", "content": prompt}]
            response = await self.openai_client.chat_completion(
                messages=messages,
                temperature=0.3,
                max_tokens=2000
            )
            
            # è§£æJSONå“åº”
            response_content = response.choices[0].message.content
            try:
                # æ¸…ç†å“åº”å†…å®¹
                cleaned_content = response_content.strip()
                if cleaned_content.startswith("```json"):
                    cleaned_content = cleaned_content[7:]
                if cleaned_content.startswith("```"):
                    cleaned_content = cleaned_content[3:]
                if cleaned_content.endswith("```"):
                    cleaned_content = cleaned_content[:-3]
                cleaned_content = cleaned_content.strip()
                
                response_json = json.loads(cleaned_content)
            except json.JSONDecodeError as e:
                await emit_error(
                    shared, 
                    f"âŒ å¤§æ¨¡å‹è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSON: {response_content[:500]}... é”™è¯¯: {str(e)}"
                )
                return []
            
            # è§£æå¤§æ¨¡å‹å“åº”
            selected_prefabs = await self._parse_llm_filter_response(response_json, prefabs, shared)
            
            return selected_prefabs
            
        except Exception as e:
            await emit_error(shared, f"âŒ å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
            return []
    
    def _build_filter_prompt(
        self, 
        query: str, 
        prefabs: List[Dict[str, Any]], 
        top_k: int,
        language: str
    ) -> str:
        """æ„å»ºå¤§æ¨¡å‹ç­›é€‰çš„æç¤ºè¯"""
        # æ„å»ºé¢„åˆ¶ä»¶ä¿¡æ¯åˆ—è¡¨
        prefabs_info = []
        for i, prefab in enumerate(prefabs):
            prefab_info = {
                "index": i,
                "id": prefab["id"],
                "type": prefab["type"],
                "name": prefab["summary"],
                "description": prefab["description"][:500] + "..." if len(prefab["description"]) > 500 else prefab["description"],
                "tags": prefab.get("tags", "")
            }
            prefabs_info.append(prefab_info)
        
        # ä½¿ç”¨å¤šè¯­è¨€æ¨¡æ¿ç³»ç»Ÿè·å–æç¤ºè¯
        prompt = get_prompt(
            PromptTypes.Agent.PREFAB_RECOMMENDATION,
            language=language,
            query=query,
            prefabs_info=json.dumps(prefabs_info, ensure_ascii=False, indent=2),
            top_k=top_k,
            prefabs_count=len(prefabs)-1
        )
        
        return prompt
    
    async def _parse_llm_filter_response(
        self, 
        response: Dict[str, Any], 
        original_prefabs: List[Dict[str, Any]],
        shared: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """è§£æå¤§æ¨¡å‹ç­›é€‰å“åº”"""
        try:
            if "selected_prefabs" not in response:
                await emit_error(shared, "âš ï¸ å¤§æ¨¡å‹å“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘selected_prefabså­—æ®µ")
                return []
            
            selected_prefabs = response["selected_prefabs"]
            if not isinstance(selected_prefabs, list):
                await emit_error(shared, "âš ï¸ å¤§æ¨¡å‹å“åº”æ ¼å¼é”™è¯¯ï¼šselected_prefabsä¸æ˜¯åˆ—è¡¨")
                return []
            
            if not selected_prefabs:
                await emit_processing_status(shared, "âœ… å¤§æ¨¡å‹åˆ†æåè®¤ä¸ºæ²¡æœ‰åˆé€‚çš„é¢„åˆ¶ä»¶")
                return []
            
            filtered_prefabs = []
            used_indices = set()
            
            for item in selected_prefabs:
                if not isinstance(item, dict) or "index" not in item:
                    continue
                
                index = item["index"]
                if not isinstance(index, int) or index < 0 or index >= len(original_prefabs):
                    continue
                
                if index in used_indices:
                    continue
                
                used_indices.add(index)
                prefab = original_prefabs[index].copy()
                prefab["llm_reason"] = item.get("reason", "")
                prefab["llm_selected"] = True
                filtered_prefabs.append(prefab)
            
            await emit_processing_status(
                shared, 
                f"âœ… å¤§æ¨¡å‹ç­›é€‰è§£ææˆåŠŸï¼Œç­›é€‰å‡º {len(filtered_prefabs)} ä¸ªé¢„åˆ¶ä»¶"
            )
            if "analysis" in response:
                await emit_processing_status(shared, f"ğŸ“ å¤§æ¨¡å‹åˆ†æ: {response['analysis']}")
            
            return filtered_prefabs
            
        except Exception as e:
            await emit_error(shared, f"âŒ è§£æå¤§æ¨¡å‹å“åº”å¤±è´¥: {str(e)}")
            return []

