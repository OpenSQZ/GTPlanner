"""
ReAct Orchestrator Node

åŸºäºFunction Callingçš„ReActä¸»æ§åˆ¶å™¨èŠ‚ç‚¹ï¼Œé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ã€‚
è´Ÿè´£å¤„ç†å•æ¬¡ReActæ¨ç†å’Œå†³ç­–é€»è¾‘ã€‚
"""

## âœ… å·²å®ç°ï¼šå¤„ç†contentä¸­åŒ…å«æ ‡ç­¾çš„æ–¹å¼ - ä½¿ç”¨ContentToolCallAdapteré€‚é…å™¨
from typing import Dict, List, Any, Optional
from pocketflow import AsyncNode

# å¯¼å…¥OpenAI SDKå’ŒFunction Callingå·¥å…·
from gtplanner.utils.openai_client import get_openai_client
from gtplanner.agent.function_calling import get_agent_function_definitions

# å¯¼å…¥æµå¼å“åº”ç±»å‹
from gtplanner.agent.streaming.stream_types import StreamCallbackType
from gtplanner.agent.streaming.stream_interface import StreamingSession

# å¯¼å…¥é‡æ„åçš„ç»„ä»¶
from .constants import (
    ErrorMessages,
    DefaultValues
)

# å¯¼å…¥å¤šè¯­è¨€æç¤ºè¯ç³»ç»Ÿ
from gtplanner.agent.prompts import get_prompt, PromptTypes

from .tool_executor import ToolExecutor




class ReActOrchestratorNode(AsyncNode):
    """ReActä¸»æ§åˆ¶å™¨èŠ‚ç‚¹ - æ¨¡å—åŒ–è®¾è®¡"""

    def __init__(self):
        super().__init__()
        self.name = "ReActOrchestratorNode"
        self.description = "åŸºäºFunction Callingçš„æ¨¡å—åŒ–ReActä¸»æ§åˆ¶å™¨èŠ‚ç‚¹"

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = get_openai_client()

        # è·å–å¯ç”¨çš„Function Callingå·¥å…·
        self.available_tools = get_agent_function_definitions()

        # åˆå§‹åŒ–ç»„ä»¶
        self.tool_executor = ToolExecutor()

    async def prep_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚æ­¥å‡†å¤‡ReActæ‰§è¡Œç¯å¢ƒï¼ˆæ— çŠ¶æ€ç‰ˆæœ¬ï¼‰"""
        try:
            return {
                "success": True,
                "shared_data": shared
            }

        except Exception as e:
            return {"error": f"{ErrorMessages.REACT_PREP_FAILED}: {str(e)}"}

    async def exec_async(self, prep_result: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚æ­¥ReActæ¨ç†å’Œå†³ç­–é€»è¾‘ - åŸºäºFunction Callingï¼ˆæ”¯æŒæµå¼å“åº”ï¼‰"""

        try:
            if "error" in prep_result:
                raise ValueError(prep_result["error"])

            shared_data = prep_result.get("shared_data", {})

            # ç›´æ¥ä½¿ç”¨dialogue_historyä¸­çš„messagesï¼ˆå®Œæ•´æˆ–å‹ç¼©è¿‡çš„èŠå¤©è®°å½•ï¼‰
            dialogue_history = shared_data.get("dialogue_history", {})
            messages = dialogue_history.get("messages", [])

            # ä½¿ç”¨Function Callingæ‰§è¡Œï¼ˆä¼ é€’shared_dataä½œä¸ºsharedå‚æ•°ï¼‰
            result = await self._execute_with_function_calling(messages, shared_data)

            return result

        except Exception as e:
            # åœ¨æ²¡æœ‰sharedå­—å…¸è®¿é—®æƒé™æ—¶ï¼Œåªèƒ½è¿”å›é”™è¯¯
            return {
                "error": f"{ErrorMessages.REACT_EXEC_FAILED}: {str(e)}",
                "user_message": ErrorMessages.GENERIC_ERROR,
                "decision_success": False,
                "exec_error": str(e)  # æ·»åŠ è¯¦ç»†é”™è¯¯ä¿¡æ¯ä¾›post_asyncå¤„ç†
            }

    async def post_async(
        self,
        shared: Dict[str, Any],
        prep_res: Dict[str, Any],
        exec_res: Dict[str, Any]
    ) -> str:
        """å¼‚æ­¥æ›´æ–°å…±äº«çŠ¶æ€ï¼ˆæ— çŠ¶æ€ç‰ˆæœ¬ï¼‰"""
        try:
            if "error" in exec_res:
                # è®°å½•execé˜¶æ®µçš„é”™è¯¯åˆ°shared
                if "errors" not in shared:
                    shared["errors"] = []
                shared["errors"].append({
                    "source": "ReActOrchestratorNode.exec",
                    "error": exec_res.get("exec_error", exec_res["error"]),
                    "timestamp": __import__('time').time()
                })
                shared["react_error"] = exec_res["error"]
                return "error"

            # æ›´æ–°ReActå¾ªç¯è®¡æ•°
            self._increment_react_cycle(shared)

            # è·å–æ‰§è¡Œç»“æœ
            tool_calls = exec_res.get("tool_calls", [])

            # æ³¨æ„ï¼šassistantæ¶ˆæ¯å·²ç»åœ¨_unified_function_calling_cycleä¸­è¢«æ·»åŠ åˆ°shared["new_messages"]
            # è¿™é‡Œä¸éœ€è¦å†æ¬¡æ·»åŠ ï¼Œé¿å…é‡å¤ä¿å­˜

            # å¤„ç†å·¥å…·è°ƒç”¨ç»“æœï¼ˆæå–åˆ°sharedå­—å…¸çš„å·¥å…·æ‰§è¡Œç»“æœå­—æ®µï¼‰
            if tool_calls:
                self._process_tool_calls(shared, tool_calls)

            # ç®€åŒ–è·¯ç”±ï¼šæ€»æ˜¯ç­‰å¾…ç”¨æˆ·ï¼Œè®©LLMåœ¨å›å¤ä¸­è‡ªç„¶å¼•å¯¼ä¸‹ä¸€æ­¥
            return "wait_for_user"

        except Exception as e:
            # è®°å½•é”™è¯¯åˆ°sharedå­—å…¸ï¼Œä¸æ‰“å°åˆ°æ§åˆ¶å°
            if "errors" not in shared:
                shared["errors"] = []
            shared["errors"].append({
                "source": "ReActOrchestratorNode.post",
                "error": str(e),
                "timestamp": __import__('time').time()
            })
            shared["react_post_error"] = str(e)
            return "error"



    def _add_assistant_message(self, shared: Dict[str, Any], message: str, tool_calls: Optional[List[Dict[str, Any]]]) -> None:
        """æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°é¢„ç•™å­—æ®µï¼ˆOpenAI APIæ ‡å‡†æ ¼å¼ï¼‰"""
        from gtplanner.agent.context_types import create_assistant_message

        assistant_message = create_assistant_message(
            content=message,
            tool_calls=tool_calls or None
        )

        # æ·»åŠ åˆ°é¢„ç•™å­—æ®µ
        if "new_messages" not in shared:
            shared["new_messages"] = []

        shared["new_messages"].append(assistant_message)

    def _add_tool_message(self, shared: Dict[str, Any], tool_call_id: str, content: str) -> None:
        """æ·»åŠ toolæ¶ˆæ¯åˆ°é¢„ç•™å­—æ®µï¼ˆOpenAI APIæ ‡å‡†æ ¼å¼ï¼‰"""
        from gtplanner.agent.context_types import create_tool_message

        tool_message = create_tool_message(
            content=content,
            tool_call_id=tool_call_id
        )

        # æ·»åŠ åˆ°é¢„ç•™å­—æ®µ
        if "new_messages" not in shared:
            shared["new_messages"] = []

        shared["new_messages"].append(tool_message)

    def _process_tool_calls(self, shared: Dict[str, Any], tool_calls: List[Dict[str, Any]]) -> None:
        """å¤„ç†å·¥å…·è°ƒç”¨ç»“æœï¼ˆæå–åˆ°sharedå­—å…¸ï¼‰"""
        for tool_call in tool_calls:
            tool_name = tool_call.get("tool_name")
            tool_result = tool_call.get("result")

            if tool_name and tool_result:
                # æå–å·¥å…·æ‰§è¡Œç»“æœåˆ°ä¸»sharedå­—å…¸
                self._extract_tool_execution_results(shared, tool_name, tool_result)

    def _extract_tool_execution_results(self, shared: Dict[str, Any], tool_name: str, tool_result: Dict[str, Any]) -> None:
        """æå–å·¥å…·æ‰§è¡Œç»“æœåˆ°ä¸»sharedå­—å…¸"""
        # æ ¹æ®å·¥å…·åç§°æå–ç‰¹å®šçš„ç»“æœ
        if tool_name == "prefab_recommend" and tool_result.get("success"):
            result_data = tool_result.get("result", {})
            recommended_prefabs = result_data.get("recommended_prefabs")
            if recommended_prefabs:
                shared["recommended_prefabs"] = recommended_prefabs
        
        elif tool_name == "search_prefabs" and tool_result.get("success"):
            result_data = tool_result.get("result", {})
            prefabs = result_data.get("prefabs")
            if prefabs:
                shared["recommended_prefabs"] = prefabs

        elif tool_name == "research" and tool_result.get("success"):
            result_data = tool_result.get("result", {})
            # researchå·¥å…·çš„resultç›´æ¥å°±æ˜¯research_findingså†…å®¹
            if result_data:
                shared["research_findings"] = result_data

        elif tool_name == "short_planning" and tool_result.get("success"):
            result_data = tool_result.get("result", {})
            # short_planningå·¥å…·çš„resultç›´æ¥å°±æ˜¯è§„åˆ’å†…å®¹
            if result_data:
                shared["short_planning"] = result_data
        
        elif tool_name == "design" and tool_result.get("success"):
            # æå–è®¾è®¡æ–‡æ¡£ä¿¡æ¯ï¼ˆç”¨äºåç»­ç¼–è¾‘ï¼‰
            document_content = tool_result.get("document")
            if document_content:
                # åˆå§‹åŒ– generated_documents åˆ—è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if "generated_documents" not in shared:
                    shared["generated_documents"] = []
                
                # æ·»åŠ æ–‡æ¡£ä¿¡æ¯
                shared["generated_documents"].append({
                    "type": "design",
                    "filename": "design.md",
                    "content": document_content,
                    "tool_name": tool_name
                })
        
        elif tool_name == "database_design" and tool_result.get("success"):
            # æå–æ•°æ®åº“è®¾è®¡æ–‡æ¡£ä¿¡æ¯
            db_design_content = tool_result.get("result")
            if db_design_content:
                if "generated_documents" not in shared:
                    shared["generated_documents"] = []
                
                shared["generated_documents"].append({
                    "type": "database_design",
                    "filename": "database_design.md",
                    "content": db_design_content,
                    "tool_name": tool_name
                })

    def _increment_react_cycle(self, shared: Dict[str, Any]) -> int:
        """å¢åŠ ReActå¾ªç¯è®¡æ•°"""
        current_count = shared.get("react_cycle_count", 0)
        new_count = current_count + 1
        shared["react_cycle_count"] = new_count
        return new_count

    async def _execute_with_function_calling(
        self,
        messages: List[Dict[str, str]],
        shared: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨Function Callingæ‰§è¡ŒReActé€»è¾‘ï¼ˆç»Ÿä¸€é€’å½’æ¶æ„ï¼‰"""
        try:
            # è·å–æµå¼å“åº”å‚æ•°
            streaming_session = shared.get("streaming_session")
            streaming_callbacks = shared.get("streaming_callbacks", {})

            # å¦‚æœæœ‰æµå¼ä¼šè¯ï¼Œä½¿ç”¨ç»Ÿä¸€çš„é€’å½’å‡½æ•°
            if streaming_session and streaming_callbacks:
                return await self._unified_function_calling_cycle(
                    messages, shared, streaming_session, streaming_callbacks, recursion_depth=0
                )
            else:
                # éæµå¼å¤„ç†æš‚ä¸æ”¯æŒï¼Œè¿”å›æç¤ºä¿¡æ¯
                return {
                    "user_message": "å½“å‰ä»…æ”¯æŒæµå¼å¤„ç†æ¨¡å¼ï¼Œè¯·ç¡®ä¿æä¾›äº†æ­£ç¡®çš„æµå¼ä¼šè¯å‚æ•°ã€‚",
                    "tool_calls": [],
                    "reasoning": "éæµå¼å¤„ç†æ¨¡å¼æœªå®ç°",
                    "confidence": 0.0,
                    "decision_success": False,
                    "execution_mode": "non_streaming_not_supported"
                }

        except Exception as e:
            return {
                "user_message": "",
                "tool_calls": [],
                "reasoning": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                "confidence": 0.0,
                "decision_success": False,
                "execution_mode": "error"
            }



    async def _unified_function_calling_cycle(
        self,
        messages: List[Dict[str, Any]],
        shared: Dict[str, Any],
        streaming_session: StreamingSession,
        streaming_callbacks: Dict[str, Any],
        recursion_depth: int = 0,
        max_recursion_depth: int = 5
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„Function Callingé€’å½’å¾ªç¯å¤„ç†å™¨

        è¿™ä¸ªå‡½æ•°åˆå¹¶äº†åŸæ¥çš„_execute_with_streamingå’Œ_process_function_calling_cycleçš„åŠŸèƒ½ï¼Œ
        æ¶ˆé™¤äº†ä»£ç é‡å¤ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„é€’å½’å¤„ç†æµç¨‹ã€‚

        Args:
            messages: æ¶ˆæ¯å†å²
            shared: å…±äº«çŠ¶æ€å­—å…¸
            streaming_session: æµå¼ä¼šè¯
            streaming_callbacks: æµå¼å›è°ƒ
            recursion_depth: å½“å‰é€’å½’æ·±åº¦
            max_recursion_depth: æœ€å¤§é€’å½’æ·±åº¦é™åˆ¶

        Returns:
            æœ€ç»ˆçš„æ‰§è¡Œç»“æœ
        """
        # é˜²æ­¢æ— é™é€’å½’
        if recursion_depth >= max_recursion_depth:
            return {
                "user_message": f"å·²è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦({max_recursion_depth})ï¼Œåœæ­¢è¿›ä¸€æ­¥çš„å·¥å…·è°ƒç”¨ã€‚",
                "tool_calls": [],
                "reasoning": f"é€’å½’æ·±åº¦é™åˆ¶ï¼Œåœæ­¢åœ¨ç¬¬{recursion_depth}è½®",
                "confidence": 0.7,
                "decision_success": True,
                "execution_mode": "recursion_limit_reached"
            }

        try:
            # æ­¥éª¤1: è°ƒç”¨LLMå¹¶å¤„ç†æµå¼å“åº”
            assistant_message_content, assistant_tool_calls = await self._call_llm_with_streaming(
                messages, shared, streaming_session, streaming_callbacks
            )

            # æ­¥éª¤2: ç°åœ¨å·¥å…·è°ƒç”¨è½¬æ¢åœ¨æºå¤´è¿›è¡Œï¼Œç›´æ¥ä½¿ç”¨ç»“æœ
            # assistant_message_content å·²ç»æ˜¯è¿‡æ»¤åçš„æ˜¾ç¤ºå†…å®¹
            # assistant_tool_calls å·²ç»åŒ…å«äº†ä»contentæ ‡ç­¾è½¬æ¢çš„å·¥å…·è°ƒç”¨

            # æ­¥éª¤3: ä¿å­˜assistantæ¶ˆæ¯åˆ°sharedå­—å…¸ï¼ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹ï¼‰
            self._add_assistant_message(shared, assistant_message_content, assistant_tool_calls)

            # æ­¥éª¤4: å¤„ç†å·¥å…·è°ƒç”¨æˆ–è¿”å›æœ€ç»ˆç»“æœ
            if assistant_tool_calls:
                # å°†assistantæ¶ˆæ¯æ·»åŠ åˆ°å†å²ï¼ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹ï¼‰
                assistant_message = {
                    "role": "assistant",
                    "content": assistant_message_content,
                    "tool_calls": assistant_tool_calls
                }
                messages.append(assistant_message)

                # æ­¥éª¤5: æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_execution_results = await self._execute_tools_with_callbacks(
                    assistant_tool_calls, shared, streaming_session, streaming_callbacks
                )

                # æ­¥éª¤6: å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                self._add_tool_results_to_messages(
                    messages, assistant_tool_calls, tool_execution_results, shared
                )

                # æ­¥éª¤6: é€’å½’è°ƒç”¨å¤„ç†åç»­å“åº”
                return await self._unified_function_calling_cycle(
                    messages, shared, streaming_session, streaming_callbacks,
                    recursion_depth + 1, max_recursion_depth
                )
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå‘é€ assistant_message_end äº‹ä»¶ç„¶åè¿”å›æœ€ç»ˆç»“æœ
                if StreamCallbackType.ON_LLM_END in streaming_callbacks:
                    await streaming_callbacks[StreamCallbackType.ON_LLM_END](
                        streaming_session,
                        complete_message=assistant_message_content,
                        tool_calls=[]  # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä¼ é€’ç©ºåˆ—è¡¨
                    )

                return {
                    "user_message": assistant_message_content,
                    "tool_calls": [],
                    "reasoning": f"å®Œæˆ{recursion_depth + 1}è½®Function Callingå¾ªç¯" if recursion_depth > 0 else "LLMç›´æ¥å›å¤ï¼Œæ— éœ€å·¥å…·è°ƒç”¨",
                    "confidence": 0.9,
                    "decision_success": True,
                    "execution_mode": f"complete_depth_{recursion_depth + 1}" if recursion_depth > 0 else "direct_response"
                }

        except Exception as e:
            # åœ¨é€’å½’æ‰§è¡Œä¸­è®°å½•é”™è¯¯åˆ°shared
            if "errors" not in shared:
                shared["errors"] = []
            shared["errors"].append({
                "source": f"ReActOrchestratorNode.unified_cycle_depth_{recursion_depth}",
                "error": str(e),
                "timestamp": __import__('time').time()
            })
            return {
                "user_message": f"åœ¨ç¬¬{recursion_depth + 1}è½®Function Callingä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}",
                "tool_calls": [],
                "reasoning": f"ç¬¬{recursion_depth + 1}è½®æ‰§è¡Œå¤±è´¥",
                "confidence": 0.0,
                "decision_success": False,
                "execution_mode": "recursion_error"
            }

    async def _call_llm_with_streaming(
        self,
        messages: List[Dict[str, Any]],
        shared: Dict[str, Any],
        streaming_session: StreamingSession,
        streaming_callbacks: Dict[str, Any]
    ) -> tuple[str, List[Dict[str, Any]]]:
        """
        è°ƒç”¨LLMå¹¶å¤„ç†æµå¼å“åº”

        Returns:
            (assistant_message_content, assistant_tool_calls)
        """
        try:
            # è§¦å‘LLMå¼€å§‹å›è°ƒ
            if StreamCallbackType.ON_LLM_START in streaming_callbacks:
                await streaming_callbacks[StreamCallbackType.ON_LLM_START](streaming_session)

            # è·å–è¯­è¨€è®¾ç½®å’Œç³»ç»Ÿæç¤ºè¯
            language = shared.get("language")  # ä»sharedå­—å…¸è·å–è¯­è¨€é€‰æ‹©
            system_prompt = get_prompt(
                PromptTypes.System.ORCHESTRATOR_FUNCTION_CALLING,
                language=language
            )

            # åŠ¨æ€æ·»åŠ å¯æŸ¥çœ‹æ–‡æ¡£åˆ—è¡¨åˆ°ç³»ç»Ÿæç¤ºè¯
            generated_documents = shared.get("generated_documents", [])
            if generated_documents:
                available_docs = [doc.get("filename") for doc in generated_documents if doc.get("filename")]
                if available_docs:
                    docs_list = "\n".join([f"- {filename}" for filename in available_docs])
                    if language == "zh":
                        context_info = f"\n\n# å½“å‰ä¼šè¯ä¸Šä¸‹æ–‡\n\n## å·²ç”Ÿæˆçš„æ–‡æ¡£\nå½“å‰ä¼šè¯ä¸­å·²ç”Ÿæˆä»¥ä¸‹æ–‡æ¡£ï¼Œå¯ä½¿ç”¨ `view_document` å·¥å…·æŸ¥çœ‹ï¼š\n{docs_list}\n"
                    else:
                        context_info = f"\n\n# Current Session Context\n\n## Generated Documents\nThe following documents have been generated in this session and can be viewed using the `view_document` tool:\n{docs_list}\n"
                    system_prompt += context_info


            # ä½¿ç”¨æµå¼APIï¼ˆå¯ç”¨å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤ï¼‰
            stream = self.openai_client.chat_completion_stream(
                system_prompt=system_prompt,
                messages=messages,
                tools=self.available_tools,
                parallel_tool_calls=True,
                filter_tool_tags=True
            )

            # æ”¶é›†æµå¼å“åº”
            assistant_message_content = ""
            current_tool_calls = {}
            chunk_index = 0

            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    delta = choice.delta

                    # å¤„ç†å†…å®¹ç‰‡æ®µï¼ˆç°åœ¨å·²ç»åœ¨æºå¤´è¿‡æ»¤äº†å·¥å…·è°ƒç”¨æ ‡ç­¾ï¼‰
                    if delta.content:
                        assistant_message_content += delta.content

                        # ç›´æ¥è¾“å‡ºå·²è¿‡æ»¤çš„å†…å®¹
                        if StreamCallbackType.ON_LLM_CHUNK in streaming_callbacks:
                            await streaming_callbacks[StreamCallbackType.ON_LLM_CHUNK](
                                streaming_session,
                                chunk_content=delta.content,
                                chunk_index=chunk_index
                            )
                            chunk_index += 1

                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if delta.tool_calls:
                        for tool_call_delta in delta.tool_calls:
                            index = tool_call_delta.index
                            if index not in current_tool_calls:
                                current_tool_calls[index] = {
                                    "id": tool_call_delta.id or "",
                                    "type": "function",
                                    "function": {"name": "", "arguments": ""}
                                }
                            if tool_call_delta.id:
                                current_tool_calls[index]["id"] = tool_call_delta.id
                            if tool_call_delta.function:
                                if tool_call_delta.function.name:
                                    current_tool_calls[index]["function"]["name"] = tool_call_delta.function.name
                                if tool_call_delta.function.arguments:
                                    current_tool_calls[index]["function"]["arguments"] += tool_call_delta.function.arguments

            # æ„å»ºå·¥å…·è°ƒç”¨åˆ—è¡¨
            assistant_tool_calls = [tool_call for tool_call in current_tool_calls.values() if tool_call["id"]]

            # ğŸ› è°ƒè¯•æ—¥å¿—ï¼šæ£€æµ‹å¹¶å‘å·¥å…·è°ƒç”¨
            if len(assistant_tool_calls) > 1:
                tool_names = [tc["function"]["name"] for tc in assistant_tool_calls]
                print(f"âš ï¸ LLMè¿”å›äº† {len(assistant_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨: {tool_names}")
                print(f"ğŸ“‹ è¯¦ç»†ä¿¡æ¯:")
                for i, tc in enumerate(assistant_tool_calls):
                    print(f"  [{i}] {tc['function']['name']} - call_id: {tc['id']}")
            elif len(assistant_tool_calls) == 1:
                print(f"âœ… LLMè¿”å›äº†å•ä¸ªå·¥å…·è°ƒç”¨: {assistant_tool_calls[0]['function']['name']}")

            # è§¦å‘LLMç»“æŸå›è°ƒï¼ˆä½¿ç”¨å·²è¿‡æ»¤çš„å†…å®¹ï¼Œå¹¶ä¼ é€’ tool_calls ä¿¡æ¯ï¼‰
            if StreamCallbackType.ON_LLM_END in streaming_callbacks:
                await streaming_callbacks[StreamCallbackType.ON_LLM_END](
                    streaming_session,
                    complete_message=assistant_message_content,
                    tool_calls=assistant_tool_calls  # ä¼ é€’å·¥å…·è°ƒç”¨ä¿¡æ¯
                )

            return assistant_message_content, assistant_tool_calls

        except Exception as e:
            # LLMè°ƒç”¨å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")



    async def _execute_tools_with_callbacks(
        self,
        tool_calls: List[Dict[str, Any]],
        shared: Dict[str, Any],
        streaming_session: StreamingSession,
        streaming_callbacks: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶å¤„ç†å›è°ƒ"""
        # ğŸ› è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æ¥æ”¶åˆ°çš„å·¥å…·è°ƒç”¨
        print(f"ğŸ“ _execute_tools_with_callbacks æ¥æ”¶åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
        tool_names = [tc["function"]["name"] for tc in tool_calls]
        call_ids = [tc["id"] for tc in tool_calls]
        print(f"   å·¥å…·åˆ—è¡¨: {tool_names}")
        print(f"   Call IDs: {call_ids}")

        # æ£€æµ‹é‡å¤çš„å·¥å…·è°ƒç”¨
        if len(tool_calls) != len(set(call_ids)):
            print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°é‡å¤çš„ call_idï¼")
            for i, cid in enumerate(call_ids):
                if call_ids.count(cid) > 1:
                    print(f"   é‡å¤çš„ call_id: {cid} (å‡ºç° {call_ids.count(cid)} æ¬¡)")

        # æ£€æµ‹ç›¸åŒå·¥å…·åç§°çš„å¤šæ¬¡è°ƒç”¨
        from collections import Counter
        tool_counter = Counter(tool_names)
        for tool_name, count in tool_counter.items():
            if count > 1:
                print(f"âš ï¸ è­¦å‘Šï¼šå·¥å…· '{tool_name}' è¢«è°ƒç”¨äº† {count} æ¬¡")
                # æ‰¾å‡ºæ‰€æœ‰è¿™ä¸ªå·¥å…·çš„è°ƒç”¨è¯¦æƒ…
                for i, tc in enumerate(tool_calls):
                    if tc["function"]["name"] == tool_name:
                        import json
                        try:
                            args = json.loads(tc["function"]["arguments"])
                        except:
                            args = tc["function"]["arguments"]
                        print(f"   [{i}] call_id: {tc['id']}, args: {args}")

        # è§¦å‘å·¥å…·è°ƒç”¨å¼€å§‹å›è°ƒ
        for tool_call in tool_calls:
            if StreamCallbackType.ON_TOOL_START in streaming_callbacks:
                import json
                try:
                    arguments = json.loads(tool_call["function"]["arguments"])
                except:
                    arguments = tool_call["function"]["arguments"]

                await streaming_callbacks[StreamCallbackType.ON_TOOL_START](
                    streaming_session,
                    tool_name=tool_call["function"]["name"],
                    arguments=arguments,
                    call_id=tool_call["id"]  # ä¼ é€’LLMç”Ÿæˆçš„å·¥å…·è°ƒç”¨ID
                )

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_execution_results = await self.tool_executor.execute_tools_parallel(
            tool_calls, shared, streaming_session
        )

        # è§¦å‘å·¥å…·è°ƒç”¨ç»“æŸå›è°ƒ
        for tool_result in tool_execution_results:
            if StreamCallbackType.ON_TOOL_END in streaming_callbacks:
                await streaming_callbacks[StreamCallbackType.ON_TOOL_END](
                    streaming_session,
                    tool_name=tool_result.get("tool_name", "unknown"),
                    result=tool_result.get("result", {}),
                    execution_time=tool_result.get("execution_time", 0),
                    success=tool_result.get("success", True),
                    error_message=tool_result.get("error")
                )

        return tool_execution_results

    def _add_tool_results_to_messages(
        self,
        messages: List[Dict[str, Any]],
        tool_calls: List[Dict[str, Any]],
        tool_execution_results: List[Dict[str, Any]],
        shared: Dict[str, Any]
    ) -> None:
        """å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²å’Œsharedå­—å…¸"""
        import json

        # æå–å·¥å…·æ‰§è¡Œç»“æœåˆ°sharedå­—å…¸
        for tool_result in tool_execution_results:
            tool_name = tool_result.get("tool_name")
            if tool_name and tool_result.get("success"):
                actual_tool_result = tool_result.get("result", {})
                self._extract_tool_execution_results(shared, tool_name, actual_tool_result)

        # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
        for i, tool_result in enumerate(tool_execution_results):
            tool_call_id = tool_calls[i]["id"]
            result_content = json.dumps(tool_result.get("result", {}), ensure_ascii=False)

            tool_message = {
                "role": "tool",
                "tool_call_id": tool_call_id,
                "content": result_content
            }
            messages.append(tool_message)

            # ä¿å­˜toolæ¶ˆæ¯åˆ°sharedå­—å…¸
            self._add_tool_message(shared, tool_call_id, result_content)



