"""
OpenAI SDKå°è£…å±‚

æä¾›ç»Ÿä¸€çš„OpenAI SDKå¼‚æ­¥æ¥å£ï¼Œé›†æˆé…ç½®ç®¡ç†ã€é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶å’ŒFunction CallingåŠŸèƒ½ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- å¼‚æ­¥ API è°ƒç”¨ï¼ˆchat_completion, chat_completion_streamï¼‰
- è‡ªåŠ¨é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
- Function Calling æ”¯æŒ
- å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤ï¼ˆToolCallTagFilterï¼‰
- å¤šæ¨¡æ€æ”¯æŒï¼ˆå›¾ç‰‡ Base64 ç¼–ç ã€Vision APIï¼‰
- æ€§èƒ½ç»Ÿè®¡å’Œæ—¥å¿—è®°å½•
"""

import asyncio
import base64
import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, AsyncIterator, Callable, TypedDict, Union
import copy
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk

from gtplanner.utils.logger_config import get_openai_logger

try:
    from dynaconf import Dynaconf
    DYNACONF_AVAILABLE = True
except ImportError:
    DYNACONF_AVAILABLE = False


class Message(TypedDict):
    """
    æ¶ˆæ¯ç±»å‹å®šä¹‰
    
    æ”¯æŒæ–‡æœ¬æ¶ˆæ¯å’Œå¤šæ¨¡æ€æ¶ˆæ¯ï¼š
    - æ–‡æœ¬æ¶ˆæ¯: {"role": "user", "content": "text"}
    - å¤šæ¨¡æ€æ¶ˆæ¯: {"role": "user", "content": [{"type": "text", "text": "..."}, {"type": "image_url", "image_url": {...}}]}
    """
    role: str
    content: Union[str, List[Dict[str, Any]]]


# ============================================================================
# å¤šæ¨¡æ€å·¥å…·å‡½æ•°
# ============================================================================

def encode_image_to_base64(
    image_source: Union[str, Path, bytes],
    image_format: Optional[str] = None
) -> str:
    """
    å°†å›¾ç‰‡ç¼–ç ä¸º Base64 å­—ç¬¦ä¸²ï¼ˆData URL æ ¼å¼ï¼‰
    
    Args:
        image_source: å›¾ç‰‡æ¥æºï¼Œæ”¯æŒï¼š
            - æ–‡ä»¶è·¯å¾„ï¼ˆstr æˆ– Pathï¼‰
            - å›¾ç‰‡å­—èŠ‚æ•°æ®ï¼ˆbytesï¼‰
        image_format: å›¾ç‰‡æ ¼å¼ï¼ˆå¦‚ 'jpeg', 'png', 'gif', 'webp'ï¼‰
            - å¦‚æœä¸º Noneï¼Œä¼šå°è¯•ä»æ–‡ä»¶æ‰©å±•åæ¨æ–­
            - å¯¹äº bytes è¾“å…¥ï¼Œé»˜è®¤ä½¿ç”¨ 'jpeg'
    
    Returns:
        Base64 ç¼–ç çš„ Data URL å­—ç¬¦ä¸²ï¼ˆå¦‚ "data:image/jpeg;base64,..."ï¼‰
    
    Examples:
        >>> # ä»æ–‡ä»¶è·¯å¾„ç¼–ç 
        >>> url = encode_image_to_base64("path/to/image.jpg")
        >>> url = encode_image_to_base64(Path("path/to/image.png"))
        
        >>> # ä»å­—èŠ‚æ•°æ®ç¼–ç 
        >>> with open("image.jpg", "rb") as f:
        ...     image_bytes = f.read()
        >>> url = encode_image_to_base64(image_bytes, image_format="jpeg")
    """
    # å¤„ç†æ–‡ä»¶è·¯å¾„è¾“å…¥
    if isinstance(image_source, (str, Path)):
        image_path = Path(image_source)
        
        if not image_path.exists():
            raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        
        # ä»æ–‡ä»¶æ‰©å±•åæ¨æ–­æ ¼å¼
        if image_format is None:
            ext = image_path.suffix.lower().lstrip('.')
            # æ ‡å‡†åŒ–æ‰©å±•å
            format_mapping = {
                'jpg': 'jpeg',
                'jpeg': 'jpeg',
                'png': 'png',
                'gif': 'gif',
                'webp': 'webp',
                'bmp': 'bmp'
            }
            image_format = format_mapping.get(ext, 'jpeg')
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(image_path, 'rb') as f:
            image_data = f.read()
    
    # å¤„ç†å­—èŠ‚æ•°æ®è¾“å…¥
    elif isinstance(image_source, bytes):
        image_data = image_source
        if image_format is None:
            image_format = 'jpeg'  # é»˜è®¤æ ¼å¼
    
    else:
        raise TypeError(f"ä¸æ”¯æŒçš„å›¾ç‰‡æºç±»å‹: {type(image_source)}")
    
    # Base64 ç¼–ç 
    base64_encoded = base64.b64encode(image_data).decode('utf-8')
    
    # æ„å»º Data URL
    data_url = f"data:image/{image_format};base64,{base64_encoded}"
    
    return data_url


def create_vision_message(
    role: str,
    text: Optional[str] = None,
    image_urls: Optional[List[str]] = None,
    image_files: Optional[List[Union[str, Path]]] = None,
    image_detail: str = "auto"
) -> Message:
    """
    åˆ›å»ºæ”¯æŒå¤šæ¨¡æ€ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰çš„æ¶ˆæ¯
    
    Args:
        role: æ¶ˆæ¯è§’è‰²ï¼ˆ"user", "assistant", "system"ï¼‰
        text: æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼‰
        image_urls: å›¾ç‰‡ URL åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯ HTTP URL æˆ– Data URLï¼‰
        image_files: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¼šè‡ªåŠ¨ç¼–ç ä¸º Base64ï¼‰
        image_detail: å›¾ç‰‡ç»†èŠ‚çº§åˆ«ï¼ˆ"auto", "low", "high"ï¼‰
            - "low": ä½ç»†èŠ‚ï¼Œæ›´å¿«æ›´ä¾¿å®œ
            - "high": é«˜ç»†èŠ‚ï¼Œæ›´æ…¢æ›´è´µä½†æ›´å‡†ç¡®
            - "auto": è‡ªåŠ¨é€‰æ‹©ï¼ˆé»˜è®¤ï¼‰
    
    Returns:
        å¤šæ¨¡æ€æ¶ˆæ¯å¯¹è±¡
    
    Examples:
        >>> # çº¯æ–‡æœ¬æ¶ˆæ¯
        >>> msg = create_vision_message("user", text="æè¿°è¿™å¼ å›¾ç‰‡")
        
        >>> # æ–‡æœ¬ + å•å¼ å›¾ç‰‡ï¼ˆURLï¼‰
        >>> msg = create_vision_message(
        ...     "user",
        ...     text="è¿™æ˜¯ä»€ä¹ˆï¼Ÿ",
        ...     image_urls=["https://example.com/image.jpg"]
        ... )
        
        >>> # æ–‡æœ¬ + å•å¼ å›¾ç‰‡ï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰
        >>> msg = create_vision_message(
        ...     "user",
        ...     text="åˆ†æè¿™å¼ å›¾ç‰‡",
        ...     image_files=["./photo.jpg"]
        ... )
        
        >>> # æ–‡æœ¬ + å¤šå¼ å›¾ç‰‡ï¼ˆæ··åˆï¼‰
        >>> msg = create_vision_message(
        ...     "user",
        ...     text="æ¯”è¾ƒè¿™äº›å›¾ç‰‡",
        ...     image_urls=["https://example.com/img1.jpg"],
        ...     image_files=["./img2.jpg", "./img3.png"],
        ...     image_detail="high"
        ... )
    """
    content_parts: List[Dict[str, Any]] = []
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹
    if text:
        content_parts.append({
            "type": "text",
            "text": text
        })
    
    # æ·»åŠ å›¾ç‰‡ URL
    if image_urls:
        for url in image_urls:
            content_parts.append({
                "type": "image_url",
                "image_url": {
                    "url": url,
                    "detail": image_detail
                }
            })
    
    # æ·»åŠ æœ¬åœ°å›¾ç‰‡æ–‡ä»¶ï¼ˆè‡ªåŠ¨ç¼–ç ä¸º Base64ï¼‰
    if image_files:
        for file_path in image_files:
            base64_url = encode_image_to_base64(file_path)
            content_parts.append({
                "type": "image_url",
                "image_url": {
                    "url": base64_url,
                    "detail": image_detail
                }
            })
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼ŒæŠ›å‡ºé”™è¯¯
    if not content_parts:
        raise ValueError("å¿…é¡»æä¾›è‡³å°‘ä¸€é¡¹å†…å®¹ï¼ˆtextã€image_urls æˆ– image_filesï¼‰")
    
    # å¦‚æœåªæœ‰æ–‡æœ¬ï¼Œç®€åŒ–ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    if len(content_parts) == 1 and content_parts[0]["type"] == "text":
        return {"role": role, "content": text}
    
    # å¤šæ¨¡æ€æ ¼å¼
    return {"role": role, "content": content_parts}


class ToolCallTagFilter:
    """
    å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤å™¨å’Œè½¬æ¢å™¨ï¼ˆçŠ¶æ€æœºæ¨¡å¼ï¼‰

    ä½¿ç”¨çŠ¶æ€æœºæ¨¡å¼å¤„ç†è·¨chunkçš„å·¥å…·è°ƒç”¨æ ‡ç­¾åˆ†å‰²æƒ…å†µï¼Œ
    æ”¯æŒæ ‡ç­¾è¢«ä»»æ„åˆ†å‰²çš„è¾¹ç•Œæƒ…å†µï¼ˆå¦‚ chunk1="<tool_", chunk2="call>{"name""ï¼‰
    """

    def __init__(self):
        # çŠ¶æ€æœºçŠ¶æ€
        self.state = "NORMAL"  # NORMAL, COLLECTING_START_TAG, IN_TOOL_CALL, COLLECTING_END_TAG

        # æ ‡ç­¾å®šä¹‰
        self.start_tag = "<tool_call>"
        self.end_tag = "</tool_call>"

        # æ ‡ç­¾æ”¶é›†ç¼“å†²åŒº
        self.tag_buffer = ""
        self.tag_target = ""

        # å†…å®¹ç¼“å†²åŒº
        self.content_buffer = ""
        self.tool_call_content = ""

        # è¾“å‡ºç¼“å†²åŒº
        self.output_buffer = ""

        # æå–çš„å·¥å…·è°ƒç”¨
        self.extracted_tool_calls = []

    def process_chunk(self, content: str) -> str:
        """
        å¤„ç†æµå¼å†…å®¹å—ï¼Œä½¿ç”¨çŠ¶æ€æœºæ¨¡å¼è¿‡æ»¤å·¥å…·è°ƒç”¨æ ‡ç­¾

        Args:
            content: åŸå§‹å†…å®¹å—

        Returns:
            è¿‡æ»¤åçš„å¯æ˜¾ç¤ºå†…å®¹
        """
        if not content:
            return ""

        output = ""

        for char in content:
            if self.state == "NORMAL":
                output += self._process_normal_char(char)
            elif self.state == "COLLECTING_START_TAG":
                output += self._process_start_tag_char(char)
            elif self.state == "IN_TOOL_CALL":
                self._process_tool_call_char(char)
            elif self.state == "COLLECTING_END_TAG":
                self._process_end_tag_char(char)

        return output

    def _process_normal_char(self, char: str) -> str:
        """å¤„ç†æ­£å¸¸çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        if char == '<':
            # å¼€å§‹æ”¶é›†å¼€å§‹æ ‡ç­¾
            self.state = "COLLECTING_START_TAG"
            self.tag_buffer = '<'
            self.tag_target = self.start_tag
            return ""  # ä¸è¾“å‡ºï¼Œç­‰å¾…ç¡®è®¤æ˜¯å¦ä¸ºå·¥å…·è°ƒç”¨æ ‡ç­¾
        else:
            return char

    def _process_start_tag_char(self, char: str) -> str:
        """å¤„ç†å¼€å§‹æ ‡ç­¾æ”¶é›†çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        self.tag_buffer += char

        if len(self.tag_buffer) <= len(self.tag_target):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ ‡ç­¾
            if self.tag_buffer == self.tag_target[:len(self.tag_buffer)]:
                if self.tag_buffer == self.tag_target:
                    # å®Œæ•´åŒ¹é…å¼€å§‹æ ‡ç­¾
                    self.state = "IN_TOOL_CALL"
                    self.tool_call_content = ""
                    self.tag_buffer = ""
                    return ""  # ä¸è¾“å‡ºæ ‡ç­¾
                else:
                    # éƒ¨åˆ†åŒ¹é…ï¼Œç»§ç»­æ”¶é›†
                    return ""
            else:
                # ä¸åŒ¹é…ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹å¹¶å›åˆ°æ­£å¸¸çŠ¶æ€
                output = self.tag_buffer
                self.state = "NORMAL"
                self.tag_buffer = ""
                return output
        else:
            # è¶…å‡ºæ ‡ç­¾é•¿åº¦ï¼Œä¸åŒ¹é…ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹å¹¶å›åˆ°æ­£å¸¸çŠ¶æ€
            output = self.tag_buffer
            self.state = "NORMAL"
            self.tag_buffer = ""
            return output

    def _process_tool_call_char(self, char: str):
        """å¤„ç†å·¥å…·è°ƒç”¨å†…å®¹çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        if char == '<':
            # å¯èƒ½æ˜¯ç»“æŸæ ‡ç­¾çš„å¼€å§‹
            self.state = "COLLECTING_END_TAG"
            self.tag_buffer = '<'
            self.tag_target = self.end_tag
        else:
            self.tool_call_content += char

    def _process_end_tag_char(self, char: str):
        """å¤„ç†ç»“æŸæ ‡ç­¾æ”¶é›†çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        self.tag_buffer += char

        if len(self.tag_buffer) <= len(self.tag_target):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ ‡ç­¾
            if self.tag_buffer == self.tag_target[:len(self.tag_buffer)]:
                if self.tag_buffer == self.tag_target:
                    # å®Œæ•´åŒ¹é…ç»“æŸæ ‡ç­¾ï¼Œå®Œæˆå·¥å…·è°ƒç”¨æå–
                    self._parse_and_store_tool_call(self.tool_call_content)
                    self.state = "NORMAL"
                    self.tag_buffer = ""
                    self.tool_call_content = ""
                # éƒ¨åˆ†åŒ¹é…ï¼Œç»§ç»­æ”¶é›†
            else:
                # ä¸åŒ¹é…ï¼Œå°†ç¼“å†²çš„å†…å®¹åŠ å…¥å·¥å…·è°ƒç”¨å†…å®¹ï¼Œç»§ç»­æ”¶é›†å·¥å…·è°ƒç”¨
                self.tool_call_content += self.tag_buffer
                self.state = "IN_TOOL_CALL"
                self.tag_buffer = ""
        else:
            # è¶…å‡ºæ ‡ç­¾é•¿åº¦ï¼Œä¸åŒ¹é…ï¼Œå°†ç¼“å†²çš„å†…å®¹åŠ å…¥å·¥å…·è°ƒç”¨å†…å®¹
            self.tool_call_content += self.tag_buffer
            self.state = "IN_TOOL_CALL"
            self.tag_buffer = ""

    def finalize(self) -> str:
        """
        å®Œæˆå¤„ç†ï¼Œè¿”å›å‰©ä½™çš„å¯æ˜¾ç¤ºå†…å®¹

        Returns:
            å‰©ä½™çš„å¯æ˜¾ç¤ºå†…å®¹
        """
        output = ""

        # å¤„ç†æœªå®Œæˆçš„çŠ¶æ€
        if self.state == "COLLECTING_START_TAG":
            # æœªå®Œæˆçš„å¼€å§‹æ ‡ç­¾æ”¶é›†ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹
            output += self.tag_buffer
        elif self.state == "IN_TOOL_CALL":
            # æœªå®Œæˆçš„å·¥å…·è°ƒç”¨ï¼Œä¸è¾“å‡ºï¼ˆå·¥å…·è°ƒç”¨ä¸å®Œæ•´ï¼‰
            pass
        elif self.state == "COLLECTING_END_TAG":
            # æœªå®Œæˆçš„ç»“æŸæ ‡ç­¾æ”¶é›†ï¼Œå°†ç¼“å†²å†…å®¹ä½œä¸ºå·¥å…·è°ƒç”¨å†…å®¹çš„ä¸€éƒ¨åˆ†
            # ä½†ç”±äºå·¥å…·è°ƒç”¨æœªå®Œæˆï¼Œä¸è¾“å‡º
            pass

        # é‡ç½®çŠ¶æ€
        self.state = "NORMAL"
        self.tag_buffer = ""
        self.tool_call_content = ""

        return output

    def _parse_and_store_tool_call(self, tool_call_content: str) -> None:
        """
        è§£æå·¥å…·è°ƒç”¨å†…å®¹å¹¶å­˜å‚¨ä¸ºæ ‡å‡†æ ¼å¼

        Args:
            tool_call_content: å·¥å…·è°ƒç”¨çš„JSONå†…å®¹
        """
        import json
        import uuid

        try:
            # è§£æJSONå†…å®¹
            tool_call_data = json.loads(tool_call_content)

            # éªŒè¯å¿…éœ€å­—æ®µ
            if "name" not in tool_call_data:
                return

            # ç”Ÿæˆå”¯ä¸€çš„call_id
            call_id = f"call_{uuid.uuid4().hex[:8]}"

            # ç¡®ä¿argumentsæ˜¯å­—ç¬¦ä¸²æ ¼å¼
            arguments = tool_call_data.get("arguments", {})
            if isinstance(arguments, dict):
                arguments_str = json.dumps(arguments, ensure_ascii=False)
            else:
                arguments_str = str(arguments)

            # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„å·¥å…·è°ƒç”¨
            standard_tool_call = {
                "id": call_id,
                "type": "function",
                "function": {
                    "name": tool_call_data["name"],
                    "arguments": arguments_str
                }
            }

            self.extracted_tool_calls.append(standard_tool_call)

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            # è§£æå¤±è´¥ï¼Œå¿½ç•¥è¿™ä¸ªå·¥å…·è°ƒç”¨
            pass

    def get_extracted_tool_calls(self) -> list:
        """
        è·å–æå–çš„å·¥å…·è°ƒç”¨åˆ—è¡¨

        Returns:
            æ ‡å‡†æ ¼å¼çš„å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        return self.extracted_tool_calls.copy()




class SimpleOpenAIConfig:
    """ç®€åŒ–çš„OpenAIé…ç½®ç±»"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "gpt-4",
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        timeout: float = 120.0,
        max_retries: int = 3,
        retry_delay: float = 2.0,
        log_requests: bool = True,
        log_responses: bool = True,
        function_calling_enabled: bool = True,
        tool_choice: str = "auto",
    ):
        # å°è¯•ä» settings.toml åŠ è½½é…ç½®
        settings = self._load_settings()

        self.api_key = api_key or self._get_setting(settings, "llm.api_key") or os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
        self.base_url = base_url or self._get_setting(settings, "llm.base_url") or os.getenv("OPENAI_BASE_URL") or os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
        self.model = self._get_setting(settings, "llm.model") or os.getenv("OPENAI_MODEL") or os.getenv("LLM_MODEL", model)
        self.temperature = self._get_setting(settings, "llm.temperature", temperature)
        self.max_tokens = self._get_setting(settings, "llm.max_tokens", max_tokens)
        self.timeout = self._get_setting(settings, "llm.timeout", timeout)
        self.max_retries = self._get_setting(settings, "llm.max_retries", max_retries)
        self.retry_delay = self._get_setting(settings, "llm.retry_delay", retry_delay)
        self.log_requests = self._get_setting(settings, "llm.log_requests", log_requests)
        self.log_responses = self._get_setting(settings, "llm.log_responses", log_responses)
        self.function_calling_enabled = self._get_setting(settings, "llm.function_calling_enabled", function_calling_enabled)
        self.tool_choice = self._get_setting(settings, "llm.tool_choice", tool_choice)

        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or configure llm.api_key in settings.toml.")

    def _load_settings(self):
        """åŠ è½½ Dynaconf è®¾ç½®"""
        if not DYNACONF_AVAILABLE:
            return None

        try:
            return Dynaconf(
                settings_files=["settings.toml", "settings.local.toml", ".secrets.toml"],
                environments=True,
                env_switcher="ENV_FOR_DYNACONF",
                load_dotenv=True,
            )
        except Exception:
            return None

    def _get_setting(self, settings, key: str, default: Any = None) -> Any:
        """ä»è®¾ç½®ä¸­è·å–å€¼"""
        if settings is None:
            return default

        try:
            return settings.get(key, default)
        except Exception:
            return default

    def to_openai_client_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºOpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å‚æ•°"""
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
            "timeout": self.timeout,
            "max_retries": self.max_retries,
        }

    def to_chat_completion_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºchat completionè°ƒç”¨å‚æ•°"""
        kwargs = {
            "model": self.model,
            "temperature": self.temperature,
        }

        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        return kwargs


class OpenAIClientError(Exception):
    """OpenAIå®¢æˆ·ç«¯é”™è¯¯åŸºç±»"""
    pass


class OpenAIRateLimitError(OpenAIClientError):
    """APIé€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass


class OpenAITimeoutError(OpenAIClientError):
    """APIè¶…æ—¶é”™è¯¯"""
    pass


class OpenAIRetryableError(OpenAIClientError):
    """å¯é‡è¯•çš„APIé”™è¯¯"""
    pass


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay

    async def execute_with_retry(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œå‡½æ•°å¹¶åœ¨å¤±è´¥æ—¶é‡è¯•

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)

            except Exception as e:
                last_error = e

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(e, attempt):
                    break

                # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = self._calculate_delay(attempt)

                # ä½¿ç”¨æ—¥å¿—è®°å½•é‡è¯•ä¿¡æ¯
                from gtplanner.utils.logger_config import get_logger
                logger = get_logger("retry_manager")
                logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries + 1}): {e}")
                logger.info(f"ğŸ”„ ç­‰å¾… {delay:.1f}ç§’åé‡è¯•...")

                await asyncio.sleep(delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise last_error

    def _should_retry(self, error: Exception, attempt: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•

        Args:
            error: é”™è¯¯å¯¹è±¡
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            æ˜¯å¦åº”è¯¥é‡è¯•
        """
        if attempt >= self.max_retries:
            return False

        error_str = str(error).lower()

        # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
        retryable_errors = [
            "rate_limit",
            "timeout",
            "connection",
            "network",
            "server_error",
            "503",
            "502",
            "500",
            "429"
        ]

        return any(err in error_str for err in retryable_errors)

    def _calculate_delay(self, attempt: int) -> float:
        """
        è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰

        Args:
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        import random

        # æŒ‡æ•°é€€é¿
        delay = self.base_delay * (2 ** attempt)

        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆÂ±25%ï¼‰
        jitter = delay * 0.25 * (random.random() * 2 - 1)

        return max(0.1, delay + jitter)


class OpenAIClient:
    """OpenAI SDKå°è£…å®¢æˆ·ç«¯"""

    def __init__(self, config: Optional[SimpleOpenAIConfig] = None):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯

        Args:
            config: OpenAIé…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or SimpleOpenAIConfig()

        # è·å–æ—¥å¿—å™¨ï¼ˆä¼šè‡ªåŠ¨åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼‰
        self.logger = get_openai_logger()

        # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
        client_kwargs = self.config.to_openai_client_kwargs()
        self.async_client = AsyncOpenAI(**client_kwargs)

        # åˆ›å»ºé‡è¯•ç®¡ç†å™¨
        self.retry_manager = RetryManager(
            max_retries=self.config.max_retries,
            base_delay=self.config.retry_delay
        )

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }


        # è®°å½•åˆå§‹åŒ–æ—¥å¿—
        self.logger.info(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.config.model}, åŸºç¡€URL: {self.config.base_url}")

    def _prepare_messages(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None
    ) -> List[Message]:
        """
        å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼Œç»Ÿä¸€å¤„ç†ç³»ç»Ÿæç¤ºè¯å’Œå…¨å±€ç³»ç»Ÿæç¤ºè¯

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            å‡†å¤‡å¥½çš„æ¶ˆæ¯åˆ—è¡¨
        """
        prepared_messages = []

        # æ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            prepared_messages.append({"role": "system", "content": system_prompt})

        # æ·»åŠ å¯¹è¯æ¶ˆæ¯
        if messages:
            prepared_messages.extend(messages)

        return prepared_messages

    def _prepare_request_params(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        stream: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‡†å¤‡APIè¯·æ±‚å‚æ•°

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            stream: æ˜¯å¦æµå¼å“åº”
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å‡†å¤‡å¥½çš„è¯·æ±‚å‚æ•°
        """
        # å‡†å¤‡æ¶ˆæ¯
        prepared_messages = self._prepare_messages(messages, system_prompt)

        # åˆå¹¶é…ç½®å‚æ•°
        params = self.config.to_chat_completion_kwargs()

        # è¿‡æ»¤æ‰å†…éƒ¨å‚æ•°ï¼Œé¿å…ä¼ é€’ç»™OpenAI API
        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ['filter_tool_tags']}
        params.update(filtered_kwargs)
        params["messages"] = prepared_messages

        if stream:
            params["stream"] = True

        # æ·»åŠ å·¥å…·æ”¯æŒ
        if tools and self.config.function_calling_enabled:
            params["tools"] = tools
            if "tool_choice" not in params:
                params["tool_choice"] = self.config.tool_choice

        return params

    def _update_success_stats(self, response: Any) -> None:
        """æ›´æ–°æˆåŠŸç»Ÿè®¡ä¿¡æ¯"""
        self.stats["successful_requests"] += 1
        if hasattr(response, 'usage') and response.usage:
            self.stats["total_tokens"] += response.usage.total_tokens

    def _update_failure_stats(self) -> None:
        """æ›´æ–°å¤±è´¥ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["failed_requests"] += 1



    async def chat_completion(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> ChatCompletion:
        """
        å¼‚æ­¥èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            èŠå¤©å®Œæˆå“åº”
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion", params)

            # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡ŒAPIè°ƒç”¨
            async def _api_call():
                return await self.async_client.chat.completions.create(**params)

            response = await self.retry_manager.execute_with_retry(_api_call)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_success_stats(response)

            # è®°å½•å“åº”æ—¥å¿—
            self._log_response("chat_completion", response)

            return response

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time
    
    async def chat_completion_stream(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        filter_tool_tags: bool = False,
        **kwargs
    ) -> AsyncIterator[ChatCompletionChunk]:
        """
        å¼‚æ­¥æµå¼èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            filter_tool_tags: æ˜¯å¦è¿‡æ»¤å·¥å…·è°ƒç”¨æ ‡ç­¾ï¼ˆé»˜è®¤Falseï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            èŠå¤©å®Œæˆæµå¼å“åº”å—ï¼ˆå¦‚æœå¯ç”¨filter_tool_tagsï¼Œdelta.contentå°†è¢«è¿‡æ»¤ï¼‰
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # æå–filter_tool_tagså‚æ•°ï¼Œé¿å…ä¼ é€’ç»™OpenAI API
            filter_tool_tags_param = filter_tool_tags

        

            # å‡†å¤‡è¯·æ±‚å‚æ•°ï¼ˆä¸åŒ…å«filter_tool_tagsï¼‰
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                stream=True,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion_stream", params)

            # æ‰§è¡Œæµå¼APIè°ƒç”¨
            stream = await self.async_client.chat.completions.create(**params)

            chunk_count = 0
            full_content = ""
            total_tokens = 0

            # åˆå§‹åŒ–å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            tag_filter = ToolCallTagFilter() if filter_tool_tags_param else None

            async for chunk in stream:
                chunk_count += 1

                # æ”¶é›†å“åº”å†…å®¹ç”¨äºæ—¥å¿—è®°å½•
                if chunk.choices and chunk.choices[0].delta.content:
                    full_content += chunk.choices[0].delta.content

                # æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
                if hasattr(chunk, 'usage') and chunk.usage:
                    total_tokens = chunk.usage.total_tokens



                # å¦‚æœå¯ç”¨äº†å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤ï¼Œå¤„ç†delta.content
                if filter_tool_tags_param and tag_filter and chunk.choices and chunk.choices[0].delta.content:
                    # åˆ›å»ºchunkçš„å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹å¯¹è±¡
                    filtered_chunk = copy.deepcopy(chunk)
                    original_content = chunk.choices[0].delta.content
                    filtered_content = tag_filter.process_chunk(original_content)

                    # æ›´æ–°å‰¯æœ¬çš„content
                    filtered_chunk.choices[0].delta.content = filtered_content

                    # å¦‚æœæå–åˆ°äº†å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°delta.tool_calls
                    extracted_calls = tag_filter.get_extracted_tool_calls()
                    if extracted_calls and not filtered_chunk.choices[0].delta.tool_calls:
                        # å°†æå–çš„å·¥å…·è°ƒç”¨è½¬æ¢ä¸ºdeltaæ ¼å¼
                        tool_call_deltas = []
                        for i, tool_call in enumerate(extracted_calls):
                            from openai.types.chat.chat_completion_chunk import ChoiceDeltaToolCall, ChoiceDeltaToolCallFunction

                            tool_call_delta = ChoiceDeltaToolCall(
                                index=i,
                                id=tool_call["id"],
                                function=ChoiceDeltaToolCallFunction(
                                    name=tool_call["function"]["name"],
                                    arguments=tool_call["function"]["arguments"]
                                ),
                                type="function"
                            )
                            tool_call_deltas.append(tool_call_delta)

                        filtered_chunk.choices[0].delta.tool_calls = tool_call_deltas
                        # æ¸…ç©ºå·²å¤„ç†çš„å·¥å…·è°ƒç”¨ï¼Œé¿å…é‡å¤
                        tag_filter.extracted_tool_calls.clear()

                    # æå‰ç»“æŸï¼šå¦‚æœæ”¶åˆ°finish_reasonï¼Œå…ˆè¾“å‡ºå†ç»ˆæ­¢å¾ªç¯
                    finish_reason = None
                    try:
                        finish_reason = chunk.choices[0].finish_reason
                    except Exception:
                        finish_reason = None

                    yield filtered_chunk

                    if finish_reason is not None:
                        break
                else:
                    # æå‰ç»“æŸï¼šå¦‚æœæ”¶åˆ°finish_reasonï¼Œå…ˆè¾“å‡ºå†ç»ˆæ­¢å¾ªç¯
                    finish_reason = None
                    try:
                        finish_reason = chunk.choices[0].finish_reason
                    except Exception:
                        finish_reason = None

                    yield chunk

                    if finish_reason is not None:
                        break

            # å¦‚æœå¯ç”¨äº†è¿‡æ»¤ï¼Œå¤„ç†å‰©ä½™çš„å†…å®¹
            if filter_tool_tags_param and tag_filter:
                remaining_content = tag_filter.finalize()
                if remaining_content:
                    # åˆ›å»ºä¸€ä¸ªåŒ…å«å‰©ä½™å†…å®¹çš„chunk
                    from openai.types.chat.chat_completion_chunk import ChatCompletionChunk, Choice, ChoiceDelta

                    # åˆ›å»ºæœ€åä¸€ä¸ªchunkæ¥è¾“å‡ºå‰©ä½™å†…å®¹
                    final_choice = Choice(
                        delta=ChoiceDelta(content=remaining_content),
                        index=0,
                        finish_reason=None
                    )
                    final_chunk = ChatCompletionChunk(
                        id="filtered_final",
                        choices=[final_choice],
                        created=int(time.time()),
                        model="filtered",
                        object="chat.completion.chunk"
                    )
                    yield final_chunk

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["successful_requests"] += 1
            if total_tokens > 0:
                self.stats["total_tokens"] += total_tokens

            # è®°å½•å“åº”æ—¥å¿—ï¼ˆæµå¼å“åº”ï¼‰
            self._log_stream_response("chat_completion_stream", chunk_count, full_content)

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time




    
    def _handle_error(self, error: Exception) -> OpenAIClientError:
        """
        å¤„ç†å’Œè½¬æ¢é”™è¯¯

        Args:
            error: åŸå§‹é”™è¯¯

        Returns:
            è½¬æ¢åçš„é”™è¯¯
        """
        import openai

        # OpenAI SDKç‰¹å®šé”™è¯¯
        if isinstance(error, openai.RateLimitError):
            return OpenAIRateLimitError(f"API rate limit exceeded: {error}")

        if isinstance(error, openai.APITimeoutError):
            return OpenAITimeoutError(f"API request timeout: {error}")

        if isinstance(error, openai.APIConnectionError):
            return OpenAIRetryableError(f"API connection error: {error}")

        if isinstance(error, openai.InternalServerError):
            return OpenAIRetryableError(f"Internal server error: {error}")

        if isinstance(error, openai.BadRequestError):
            return OpenAIClientError(f"Bad request: {error}")

        if isinstance(error, openai.AuthenticationError):
            return OpenAIClientError(f"Authentication failed: {error}")

        if isinstance(error, openai.PermissionDeniedError):
            return OpenAIClientError(f"Permission denied: {error}")

        if isinstance(error, openai.NotFoundError):
            return OpenAIClientError(f"Resource not found: {error}")

        # é€šç”¨é”™è¯¯å¤„ç†
        error_message = str(error)

        # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "rate_limit" in error_message.lower() or "429" in error_message:
            return OpenAIRateLimitError(f"API rate limit exceeded: {error_message}")

        # è¶…æ—¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "timeout" in error_message.lower() or "timed out" in error_message.lower():
            return OpenAITimeoutError(f"API request timeout: {error_message}")

        # ç½‘ç»œé”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(keyword in error_message.lower() for keyword in ["connection", "network", "dns"]):
            return OpenAIRetryableError(f"Network error: {error_message}")

        # æœåŠ¡å™¨é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(code in error_message for code in ["500", "502", "503", "504"]):
            return OpenAIRetryableError(f"Server error: {error_message}")

        # å…¶ä»–é”™è¯¯
        return OpenAIClientError(f"OpenAI API error: {error_message}")
    
    def _log_request(self, method: str, params: Dict[str, Any]) -> None:
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        if self.config.log_requests:
            self.logger.info(f"ğŸ”„ OpenAI {method} è¯·æ±‚:")
            self.logger.info(f"ï¿½ å®Œæ•´è¯·æ±‚å‚æ•°: {params}")

    def _log_response(self, method: str, response: Any) -> None:
        """è®°å½•å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} å“åº”:")
            self.logger.info(f"ğŸ“ å®Œæ•´å“åº”: {response}")

    def _log_stream_response(self, method: str, chunk_count: int, content: str = "") -> None:
        """è®°å½•æµå¼å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} æµå¼å“åº”å®Œæˆ: æ¥æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—")
            if content:
                self.logger.info(f"ğŸ“ å®Œæ•´æµå¼å“åº”å†…å®¹: {content}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def reset_stats(self) -> None:
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_global_client: Optional[OpenAIClient] = None


def get_openai_client(config: Optional[SimpleOpenAIConfig] = None) -> OpenAIClient:
    """
    è·å–å…¨å±€OpenAIå®¢æˆ·ç«¯å®ä¾‹

    Args:
        config: OpenAIé…ç½®å¯¹è±¡

    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
    """
    global _global_client

    if _global_client is None or config is not None:
        _global_client = OpenAIClient(config)

    return _global_client

