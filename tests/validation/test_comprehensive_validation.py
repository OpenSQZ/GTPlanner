"""
GTPlannerè¯·æ±‚éªŒè¯ç³»ç»Ÿç»¼åˆæµ‹è¯•

å…¨é¢æµ‹è¯•éªŒè¯ç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ‰€æœ‰éªŒè¯ç­–ç•¥çš„è¯¦ç»†æµ‹è¯•
- è´£ä»»é“¾å’Œå·¥å‚æ¨¡å¼æµ‹è¯•
- ä¸­é—´ä»¶å’Œè§‚å¯Ÿè€…ç³»ç»Ÿæµ‹è¯•
- é€‚é…å™¨å’Œé«˜çº§åŠŸèƒ½æµ‹è¯•
- é›†æˆæµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•
"""

import asyncio
import sys
import os
import time
import json

print("ğŸš€ å¼€å§‹GTPlannerè¯·æ±‚éªŒè¯ç³»ç»Ÿç»¼åˆæµ‹è¯•...")
print("=" * 80)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

# æµ‹è¯•ç»Ÿè®¡
test_stats = {
    "total_tests": 0,
    "passed_tests": 0,
    "failed_tests": 0,
    "test_results": {},
    "start_time": time.time()
}

def record_test_result(test_name: str, success: bool, details: str = ""):
    """è®°å½•æµ‹è¯•ç»“æœ"""
    test_stats["total_tests"] += 1
    if success:
        test_stats["passed_tests"] += 1
    else:
        test_stats["failed_tests"] += 1
    
    test_stats["test_results"][test_name] = {
        "success": success,
        "details": details,
        "timestamp": time.time()
    }

# æµ‹è¯•æ ¸å¿ƒæšä¸¾å’Œæ•°æ®ç»“æ„
def test_core_enums_and_structures():
    print("ğŸ§ª æµ‹è¯•æ ¸å¿ƒæšä¸¾å’Œæ•°æ®ç»“æ„...")
    
    try:
        from agent.validation.core.interfaces import ValidatorPriority
        from agent.validation.core.validation_context import ValidationMode
        from agent.validation.core.validation_result import ValidationStatus, ValidationSeverity
        
        # æµ‹è¯•æšä¸¾å®Œæ•´æ€§
        priorities = [p.name for p in ValidatorPriority]
        modes = [m.value for m in ValidationMode]
        statuses = [s.value for s in ValidationStatus]
        severities = [s.name for s in ValidationSeverity]
        
        print(f"   âœ… ValidatorPriority: {priorities}")
        print(f"   âœ… ValidationMode: {modes}")
        print(f"   âœ… ValidationStatus: {statuses}")
        print(f"   âœ… ValidationSeverity: {severities}")
        
        assert len(priorities) == 4
        assert len(modes) == 4
        assert len(statuses) == 5
        assert len(severities) == 4
        
        record_test_result("core_enums", True, "æ‰€æœ‰æ ¸å¿ƒæšä¸¾å®šä¹‰æ­£ç¡®")
        return True
        
    except Exception as e:
        record_test_result("core_enums", False, str(e))
        print(f"   âŒ æ ¸å¿ƒæšä¸¾æµ‹è¯•å¤±è´¥: {e}")
        return False

# æµ‹è¯•æ‰€æœ‰éªŒè¯ç­–ç•¥
async def test_all_validation_strategies():
    print("\nğŸ›¡ï¸ æµ‹è¯•æ‰€æœ‰éªŒè¯ç­–ç•¥...")
    
    strategy_tests = []
    
    # æµ‹è¯•å®‰å…¨éªŒè¯ç­–ç•¥
    try:
        from agent.validation.strategies.security_validator import SecurityValidationStrategy
        from agent.validation.core.validation_result import ValidationStatus
        
        strategy = SecurityValidationStrategy({
            "enable_xss_protection": True,
            "enable_sql_injection_detection": True,
            "enable_sensitive_data_detection": True
        })
        
        # XSSæµ‹è¯•ç”¨ä¾‹
        xss_cases = [
            "<script>alert('xss')</script>",
            "javascript:alert('xss')",
            "<img src=x onerror=alert('xss')>",
            "<iframe src='javascript:alert(1)'></iframe>"
        ]
        
        xss_detected = 0
        for case in xss_cases:
            result = await strategy.execute(case, {})
            if not result.is_valid and any(error.code == "XSS_DETECTED" for error in result.errors):
                xss_detected += 1
        
        # SQLæ³¨å…¥æµ‹è¯•ç”¨ä¾‹
        sql_cases = [
            "'; DROP TABLE users; --",
            "1' OR '1'='1",
            "UNION SELECT * FROM passwords",
            "admin'/**/OR/**/1=1#"
        ]
        
        sql_detected = 0
        for case in sql_cases:
            result = await strategy.execute(case, {})
            if not result.is_valid and any(error.code == "SQL_INJECTION_DETECTED" for error in result.errors):
                sql_detected += 1
        
        print(f"   âœ… å®‰å…¨ç­–ç•¥: XSSæ£€æµ‹ {xss_detected}/{len(xss_cases)}, SQLæ³¨å…¥æ£€æµ‹ {sql_detected}/{len(sql_cases)}")
        strategy_tests.append(xss_detected >= 3 and sql_detected >= 3)
        
    except Exception as e:
        print(f"   âŒ å®‰å…¨ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        strategy_tests.append(False)
    
    # æµ‹è¯•å¤§å°éªŒè¯ç­–ç•¥
    try:
        from agent.validation.strategies.size_validator import SizeValidationStrategy
        
        strategy = SizeValidationStrategy({
            "max_string_length": 50,
            "max_json_depth": 3,
            "max_array_length": 5
        })
        
        # å¤§å°æµ‹è¯•ç”¨ä¾‹
        size_cases = [
            ("A" * 100, "STRING_LENGTH_EXCEEDED"),  # è¶…é•¿å­—ç¬¦ä¸²
            ({"a": {"b": {"c": {"d": "deep"}}}}, "JSON_DEPTH_EXCEEDED"),  # æ·±åº¦åµŒå¥—
            ({"items": list(range(10))}, "ARRAY_LENGTH_EXCEEDED")  # é•¿æ•°ç»„
        ]
        
        size_detected = 0
        for case_data, expected_error in size_cases:
            result = await strategy.execute(case_data, {})
            if not result.is_valid and any(error.code == expected_error for error in result.errors):
                size_detected += 1
        
        print(f"   âœ… å¤§å°ç­–ç•¥: é™åˆ¶æ£€æµ‹ {size_detected}/{len(size_cases)}")
        strategy_tests.append(size_detected >= 2)
        
    except Exception as e:
        print(f"   âŒ å¤§å°ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        strategy_tests.append(False)
    
    # æµ‹è¯•æ ¼å¼éªŒè¯ç­–ç•¥
    try:
        from agent.validation.strategies.format_validator import FormatValidationStrategy
        
        strategy = FormatValidationStrategy({
            "validate_required_fields": True,
            "strict_field_types": True
        })
        
        # æ ¼å¼æµ‹è¯•ç”¨ä¾‹
        valid_data = {
            "session_id": "test_session",
            "dialogue_history": [{"role": "user", "content": "Hello", "timestamp": "2023-01-01T12:00:00Z"}],
            "tool_execution_results": {},
            "session_metadata": {}
        }
        
        invalid_data = {"session_id": 123}  # ç¼ºå°‘å­—æ®µï¼Œç±»å‹é”™è¯¯
        
        valid_result = await strategy.execute(valid_data, {})
        invalid_result = await strategy.execute(invalid_data, {})
        
        print(f"   âœ… æ ¼å¼ç­–ç•¥: æœ‰æ•ˆæ•°æ®é€šè¿‡={valid_result.is_valid}, æ— æ•ˆæ•°æ®æ‹’ç»={not invalid_result.is_valid}")
        strategy_tests.append(valid_result.is_valid and not invalid_result.is_valid)
        
    except Exception as e:
        print(f"   âŒ æ ¼å¼ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        strategy_tests.append(False)
    
    # æµ‹è¯•ä¼šè¯éªŒè¯ç­–ç•¥
    try:
        from agent.validation.strategies.session_validator import SessionValidationStrategy
        
        strategy = SessionValidationStrategy({"validate_session_id_format": True})
        
        # ä¼šè¯IDæµ‹è¯•ç”¨ä¾‹
        session_cases = [
            ("valid_session_123", True),
            ("550e8400-e29b-41d4-a716-446655440000", True),  # UUIDæ ¼å¼
            ("!@#$%^&*()", False),  # æ— æ•ˆå­—ç¬¦
            ("", False)  # ç©ºID
        ]
        
        session_results = []
        for session_id, should_pass in session_cases:
            data = {"session_id": session_id} if session_id else {}
            result = await strategy.execute(data, {})
            session_results.append(result.is_valid == should_pass)
        
        session_passed = sum(session_results)
        print(f"   âœ… ä¼šè¯ç­–ç•¥: æ ¼å¼éªŒè¯ {session_passed}/{len(session_cases)}")
        strategy_tests.append(session_passed >= 3)
        
    except Exception as e:
        print(f"   âŒ ä¼šè¯ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        strategy_tests.append(False)
    
    passed_strategies = sum(strategy_tests)
    total_strategies = len(strategy_tests)
    
    record_test_result("all_strategies", passed_strategies == total_strategies, 
                      f"ç­–ç•¥æµ‹è¯•é€šè¿‡ç‡: {passed_strategies}/{total_strategies}")
    
    print(f"   ğŸ‰ éªŒè¯ç­–ç•¥æµ‹è¯•å®Œæˆ: {passed_strategies}/{total_strategies} é€šè¿‡")
    return passed_strategies == total_strategies

# æµ‹è¯•è´£ä»»é“¾å’Œå·¥å‚æ¨¡å¼
async def test_chain_and_factory_patterns():
    print("\nğŸ”— æµ‹è¯•è´£ä»»é“¾å’Œå·¥å‚æ¨¡å¼...")
    
    try:
        from agent.validation.chains.async_validation_chain import AsyncValidationChain
        from agent.validation.factories.validator_factory import ValidatorFactory, StrategyValidator
        from agent.validation.factories.chain_factory import ValidationChainFactory
        from agent.validation.strategies.security_validator import SecurityValidationStrategy
        from agent.validation.strategies.size_validator import SizeValidationStrategy
        from agent.validation.core.validation_context import ValidationContext, ValidationMode
        
        # åˆ›å»ºéªŒè¯å™¨å·¥å‚
        factory = ValidatorFactory()
        factory.register_default_validators()
        
        # åˆ›å»ºéªŒè¯é“¾
        chain = AsyncValidationChain("comprehensive_test_chain")
        
        # æ·»åŠ éªŒè¯å™¨
        security_strategy = SecurityValidationStrategy({"enable_xss_protection": True})
        security_validator = StrategyValidator(security_strategy, "security")
        
        size_strategy = SizeValidationStrategy({"max_string_length": 100})
        size_validator = StrategyValidator(size_strategy, "size")
        
        chain.add_validator(security_validator)
        chain.add_validator(size_validator)
        
        print(f"   âœ… éªŒè¯é“¾åˆ›å»º: {chain.get_validator_count()} ä¸ªéªŒè¯å™¨")
        
        # æµ‹è¯•ä¸²è¡Œæ‰§è¡Œ
        test_data = {
            "session_id": "chain_test_session",
            "dialogue_history": [{"role": "user", "content": "Hello, world!"}],
            "tool_execution_results": {},
            "session_metadata": {}
        }
        
        context = ValidationContext.create_for_testing(
            request_data=test_data,
            validation_mode=ValidationMode.STRICT
        )
        
        serial_result = await chain.validate(context)
        print(f"   âœ… ä¸²è¡Œæ‰§è¡Œ: {serial_result.status.value}, è€—æ—¶: {serial_result.execution_time:.3f}s")
        
        # æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
        parallel_result = await chain.validate_parallel(context)
        print(f"   âœ… å¹¶è¡Œæ‰§è¡Œ: {parallel_result.status.value}, è€—æ—¶: {parallel_result.execution_time:.3f}s")
        
        # æµ‹è¯•æ¶æ„æ•°æ®
        malicious_data = {
            "session_id": "chain_test_session",
            "dialogue_history": [{"role": "user", "content": "<script>alert('xss')</script>"}],
            "tool_execution_results": {},
            "session_metadata": {}
        }
        
        malicious_context = ValidationContext.create_for_testing(request_data=malicious_data)
        malicious_result = await chain.validate(malicious_context)
        
        print(f"   âœ… æ¶æ„æ•°æ®æ£€æµ‹: {malicious_result.status.value}")
        
        # æµ‹è¯•éªŒè¯é“¾å·¥å‚
        chain_factory = ValidationChainFactory({
            "endpoints": {
                "/api/test": ["security", "size"]
            }
        })
        
        factory_chain = chain_factory.create_chain_for_endpoint("/api/test")
        if factory_chain:
            print(f"   âœ… å·¥å‚åˆ›å»ºéªŒè¯é“¾: {factory_chain.get_validator_count()} ä¸ªéªŒè¯å™¨")
        
        record_test_result("chain_factory", True, "è´£ä»»é“¾å’Œå·¥å‚æ¨¡å¼åŠŸèƒ½æ­£å¸¸")
        return True
        
    except Exception as e:
        record_test_result("chain_factory", False, str(e))
        print(f"   âŒ è´£ä»»é“¾å’Œå·¥å‚æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False

# æµ‹è¯•ä¸­é—´ä»¶å’Œè§‚å¯Ÿè€…
async def test_middleware_and_observers():
    print("\nâš™ï¸ æµ‹è¯•ä¸­é—´ä»¶å’Œè§‚å¯Ÿè€…...")
    
    try:
        from agent.validation.middleware.validation_middleware import ValidationMiddleware
        from agent.validation.observers.logging_observer import LoggingObserver
        from agent.validation.observers.metrics_observer import MetricsObserver
        from agent.validation.observers.streaming_observer import StreamingObserver
        
        # åˆ›å»ºè§‚å¯Ÿè€…
        logging_observer = LoggingObserver({"enabled": True, "log_successful_validations": True})
        metrics_observer = MetricsObserver({"enabled": True})
        
        # åˆ›å»ºæ¨¡æ‹Ÿæµå¼ä¼šè¯
        class MockStreamingSession:
            def __init__(self):
                self.session_id = "middleware_test"
                self.events = []
            
            async def emit_event(self, event):
                self.events.append(event)
        
        streaming_observer = StreamingObserver(MockStreamingSession())
        
        print(f"   âœ… è§‚å¯Ÿè€…åˆ›å»º: æ—¥å¿—ã€æŒ‡æ ‡ã€æµå¼")
        
        # æ¨¡æ‹ŸéªŒè¯æµç¨‹
        from agent.validation.core.validation_context import ValidationContext
        from agent.validation.core.validation_result import ValidationResult, ValidationStatus
        
        context = ValidationContext.create_for_testing(
            request_data={"test": "middleware"},
            user_id="middleware_user"
        )
        
        # é€šçŸ¥æ‰€æœ‰è§‚å¯Ÿè€…
        observers = [logging_observer, metrics_observer, streaming_observer]
        
        for observer in observers:
            await observer.on_validation_start(context)
        
        # æ¨¡æ‹ŸéªŒè¯æ­¥éª¤
        step_result = ValidationResult(ValidationStatus.SUCCESS)
        step_result.metrics.execution_time = 0.02
        
        for observer in observers:
            await observer.on_validation_step("test_validator", step_result)
        
        # æ¨¡æ‹ŸéªŒè¯å®Œæˆ
        final_result = ValidationResult(ValidationStatus.SUCCESS)
        final_result.request_id = context.request_id
        final_result.metrics.execution_time = 0.05
        
        for observer in observers:
            await observer.on_validation_complete(final_result)
        
        print("   âœ… è§‚å¯Ÿè€…äº‹ä»¶æµç¨‹å®Œæˆ")
        
        # æ£€æŸ¥æŒ‡æ ‡æ”¶é›†
        metrics = metrics_observer.get_current_metrics()
        print(f"   ğŸ“Š æŒ‡æ ‡æ”¶é›†: éªŒè¯æ¬¡æ•° {metrics['total_validations']}")
        
        record_test_result("middleware_observers", True, "ä¸­é—´ä»¶å’Œè§‚å¯Ÿè€…åŠŸèƒ½æ­£å¸¸")
        return True
        
    except Exception as e:
        record_test_result("middleware_observers", False, str(e))
        print(f"   âŒ ä¸­é—´ä»¶å’Œè§‚å¯Ÿè€…æµ‹è¯•å¤±è´¥: {e}")
        return False

# æµ‹è¯•é€‚é…å™¨é›†æˆ
async def test_adapters_integration():
    print("\nğŸ”Œ æµ‹è¯•é€‚é…å™¨é›†æˆ...")
    
    adapter_tests = []
    
    # æµ‹è¯•Pydanticé€‚é…å™¨
    try:
        from agent.validation.adapters.pydantic_adapter import AgentContextPydanticAdapter
        
        adapter = AgentContextPydanticAdapter()
        
        valid_context_data = {
            "session_id": "adapter_test_session",
            "dialogue_history": [
                {"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯", "timestamp": "2023-01-01T12:00:00Z"}
            ],
            "tool_execution_results": {"test": "result"},
            "session_metadata": {"language": "zh"}
        }
        
        result = await adapter.validate_agent_context(valid_context_data)
        print(f"   âœ… Pydanticé€‚é…å™¨: AgentContextéªŒè¯ {result.status.value}")
        adapter_tests.append(result.is_valid)
        
    except Exception as e:
        print(f"   âŒ Pydanticé€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        adapter_tests.append(False)
    
    # æµ‹è¯•FastAPIé€‚é…å™¨
    try:
        from agent.validation.adapters.fastapi_adapter import FastAPIValidationAdapter
        
        adapter = FastAPIValidationAdapter({"enable_request_validation": True})
        
        # æ¨¡æ‹Ÿè¯·æ±‚
        class MockRequest:
            def __init__(self):
                self.method = "POST"
                self.url = type('obj', (object,), {'path': '/api/test'})()
                self.headers = {"content-type": "application/json"}
                self.client = type('obj', (object,), {'host': '192.168.1.100'})()
                self.query_params = {}
            
            async def json(self):
                return {"session_id": "fastapi_test", "message": "Hello"}
        
        context = await adapter.create_context_from_request(MockRequest())
        print(f"   âœ… FastAPIé€‚é…å™¨: ä¸Šä¸‹æ–‡åˆ›å»º {context.request_path}")
        adapter_tests.append(context.request_path == "/api/test")
        
    except Exception as e:
        print(f"   âŒ FastAPIé€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        adapter_tests.append(False)
    
    # æµ‹è¯•SSEé€‚é…å™¨
    try:
        from agent.validation.adapters.sse_adapter import SSEValidationAdapter
        
        adapter = SSEValidationAdapter({"enable_progress_updates": True})
        
        class MockSSESession:
            def __init__(self):
                self.session_id = "sse_test"
                self.messages = []
            
            async def send_message(self, message):
                self.messages.append(message)
        
        mock_session = MockSSESession()
        enhanced_session = adapter.create_enhanced_streaming_session(mock_session)
        
        # å‘é€æµ‹è¯•äº‹ä»¶
        await enhanced_session.send_validation_progress("test_validator", "success", 50.0)
        
        print(f"   âœ… SSEé€‚é…å™¨: æµå¼ä¼šè¯å¢å¼º")
        adapter_tests.append(len(mock_session.messages) > 0)
        
    except Exception as e:
        print(f"   âŒ SSEé€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        adapter_tests.append(False)
    
    passed_adapters = sum(adapter_tests)
    total_adapters = len(adapter_tests)
    
    record_test_result("adapters", passed_adapters == total_adapters, 
                      f"é€‚é…å™¨æµ‹è¯•é€šè¿‡ç‡: {passed_adapters}/{total_adapters}")
    
    print(f"   ğŸ‰ é€‚é…å™¨æµ‹è¯•å®Œæˆ: {passed_adapters}/{total_adapters} é€šè¿‡")
    return passed_adapters == total_adapters

# æµ‹è¯•é«˜çº§åŠŸèƒ½
async def test_advanced_features():
    print("\nâš¡ æµ‹è¯•é«˜çº§åŠŸèƒ½...")
    
    advanced_tests = []
    
    # æµ‹è¯•é«˜çº§ç¼“å­˜
    try:
        from agent.validation.utils.cache_manager import ValidationCacheManager
        from agent.validation.core.validation_result import ValidationResult
        
        cache_manager = ValidationCacheManager({"enabled": True, "max_size": 10})
        
        # æµ‹è¯•ç¼“å­˜æ“ä½œ
        test_result = ValidationResult.create_success()
        test_result.request_id = "cache_test"
        
        await cache_manager.set("test_cache_key", test_result, ttl=300)
        cached_result = await cache_manager.get("test_cache_key")
        
        print(f"   âœ… é«˜çº§ç¼“å­˜: ç¼“å­˜å­˜å–æ­£å¸¸")
        advanced_tests.append(cached_result is not None)
        
    except Exception as e:
        print(f"   âŒ é«˜çº§ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        advanced_tests.append(False)
    
    # æµ‹è¯•å¢å¼ºæŒ‡æ ‡
    try:
        from agent.validation.observers.enhanced_metrics_observer import EnhancedMetricsObserver
        
        observer = EnhancedMetricsObserver({
            "enable_alerts": True,
            "enable_trend_analysis": True
        })
        
        # æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›†
        from agent.validation.core.validation_context import ValidationContext
        from agent.validation.core.validation_result import ValidationResult
        
        context = ValidationContext.create_for_testing()
        result = ValidationResult.create_success()
        result.metrics.execution_time = 0.03
        
        await observer.on_validation_complete(result)
        
        enhanced_metrics = observer.get_enhanced_metrics()
        print(f"   âœ… å¢å¼ºæŒ‡æ ‡: æ”¶é›† {enhanced_metrics['total_validations']} æ¬¡éªŒè¯")
        advanced_tests.append(enhanced_metrics['total_validations'] > 0)
        
    except Exception as e:
        print(f"   âŒ å¢å¼ºæŒ‡æ ‡æµ‹è¯•å¤±è´¥: {e}")
        advanced_tests.append(False)
    
    # æµ‹è¯•æ€§èƒ½ä¼˜åŒ–
    try:
        from agent.validation.utils.performance_optimizer import PerformanceOptimizer
        
        optimizer = PerformanceOptimizer({"max_concurrent_validations": 10})
        
        # è·å–ä¼˜åŒ–ç»Ÿè®¡
        stats = optimizer.get_optimization_stats()
        print(f"   âœ… æ€§èƒ½ä¼˜åŒ–: é…ç½®å®Œæˆ")
        advanced_tests.append(True)
        
    except Exception as e:
        print(f"   âŒ æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        advanced_tests.append(False)
    
    passed_advanced = sum(advanced_tests)
    total_advanced = len(advanced_tests)
    
    record_test_result("advanced_features", passed_advanced == total_advanced,
                      f"é«˜çº§åŠŸèƒ½æµ‹è¯•é€šè¿‡ç‡: {passed_advanced}/{total_advanced}")
    
    print(f"   ğŸ‰ é«˜çº§åŠŸèƒ½æµ‹è¯•å®Œæˆ: {passed_advanced}/{total_advanced} é€šè¿‡")
    return passed_advanced == total_advanced

# æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ ¼å¼åŒ–
def test_error_handling_and_formatting():
    print("\nğŸ¨ æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ ¼å¼åŒ–...")
    
    try:
        from agent.validation.utils.error_formatters import ErrorFormatter
        from agent.validation.core.validation_result import ValidationResult, ValidationError, ValidationSeverity
        from agent.validation.core.validation_context import ValidationContext
        
        # åˆ›å»ºé”™è¯¯æ ¼å¼åŒ–å™¨
        formatter = ErrorFormatter({
            "include_suggestions": True,
            "include_error_codes": True,
            "mask_sensitive_data": True,
            "max_error_message_length": 200
        })
        
        # åˆ›å»ºæµ‹è¯•é”™è¯¯
        errors = [
            ValidationError.create_xss_error("<script>alert('test')</script>"),
            ValidationError.create_size_error(2048, 1024, "content"),
            ValidationError.create_format_error("email", "emailæ ¼å¼", "invalid-email")
        ]
        
        error_result = ValidationResult(ValidationStatus.ERROR)
        for error in errors:
            error_result.add_error(error)
        
        # æµ‹è¯•å¤šè¯­è¨€æ ¼å¼åŒ–
        languages = ["zh", "en", "es", "fr", "ja"]
        formatted_responses = {}
        
        for lang in languages:
            context = ValidationContext.create_for_testing(language=lang)
            response = formatter.format_response(error_result, context)
            formatted_responses[lang] = response
            print(f"   âœ… {lang}è¯­è¨€æ ¼å¼åŒ–: {response['message']}")
        
        # æµ‹è¯•ç”¨æˆ·å‹å¥½æ¶ˆæ¯
        friendly_message = formatter.create_user_friendly_message(error_result, "zh")
        print(f"   âœ… ç”¨æˆ·å‹å¥½æ¶ˆæ¯: {friendly_message}")
        
        record_test_result("error_formatting", True, "é”™è¯¯å¤„ç†å’Œæ ¼å¼åŒ–åŠŸèƒ½æ­£å¸¸")
        return True
        
    except Exception as e:
        record_test_result("error_formatting", False, str(e))
        print(f"   âŒ é”™è¯¯å¤„ç†å’Œæ ¼å¼åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

# æµ‹è¯•å®Œæ•´éªŒè¯æµç¨‹
async def test_complete_validation_flow():
    print("\nğŸ”„ æµ‹è¯•å®Œæ•´éªŒè¯æµç¨‹...")
    
    try:
        # æ¨¡æ‹Ÿå®Œæ•´çš„APIè¯·æ±‚éªŒè¯æµç¨‹
        from agent.validation.factories.chain_factory import ValidationChainFactory
        from agent.validation.factories.config_factory import ConfigFactory
        from agent.validation.core.validation_context import ValidationContext
        from agent.validation.observers.logging_observer import LoggingObserver
        from agent.validation.observers.metrics_observer import MetricsObserver
        
        # åˆ›å»ºé…ç½®
        config_factory = ConfigFactory()
        validation_config = config_factory.create_from_template("standard")
        
        if not validation_config:
            validation_config = {
                "endpoints": {
                    "/api/chat/agent": ["security", "size", "format"]
                }
            }
        
        # åˆ›å»ºéªŒè¯é“¾å·¥å‚
        chain_factory = ValidationChainFactory(validation_config)
        
        # åˆ›å»ºè§‚å¯Ÿè€…
        observers = [
            LoggingObserver({"enabled": True}),
            MetricsObserver({"enabled": True})
        ]
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„è¯·æ±‚
        test_cases = [
            {
                "name": "æ­£å¸¸è¯·æ±‚",
                "data": {
                    "session_id": "normal_session",
                    "dialogue_history": [
                        {"role": "user", "content": "è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹æ•°æ®", "timestamp": "2023-01-01T12:00:00Z"}
                    ],
                    "tool_execution_results": {},
                    "session_metadata": {"language": "zh"}
                },
                "endpoint": "/api/chat/agent",
                "expected_valid": True
            },
            {
                "name": "XSSæ”»å‡»è¯·æ±‚",
                "data": {
                    "session_id": "attack_session",
                    "dialogue_history": [
                        {"role": "user", "content": "<script>alert('xss')</script>", "timestamp": "2023-01-01T12:00:00Z"}
                    ],
                    "tool_execution_results": {},
                    "session_metadata": {}
                },
                "endpoint": "/api/chat/agent",
                "expected_valid": False
            },
            {
                "name": "æ ¼å¼é”™è¯¯è¯·æ±‚",
                "data": {
                    "session_id": "format_error_session"
                    # ç¼ºå°‘å¿…éœ€å­—æ®µ
                },
                "endpoint": "/api/chat/agent",
                "expected_valid": False
            },
            {
                "name": "å¤§å°è¶…é™è¯·æ±‚",
                "data": {
                    "session_id": "size_error_session",
                    "dialogue_history": [
                        {"role": "user", "content": "A" * 20000, "timestamp": "2023-01-01T12:00:00Z"}  # è¶…é•¿å†…å®¹
                    ],
                    "tool_execution_results": {},
                    "session_metadata": {}
                },
                "endpoint": "/api/chat/agent",
                "expected_valid": False
            }
        ]
        
        flow_results = []
        
        for test_case in test_cases:
            print(f"   ğŸ§ª æµ‹è¯•: {test_case['name']}")
            
            # åˆ›å»ºéªŒè¯é“¾
            chain = chain_factory.create_chain_for_endpoint(test_case["endpoint"])
            if not chain:
                print(f"      âš ï¸ æ— éªŒè¯é“¾åŒ¹é…ç«¯ç‚¹ {test_case['endpoint']}")
                continue
            
            # åˆ›å»ºéªŒè¯ä¸Šä¸‹æ–‡
            context = ValidationContext.create_for_testing(
                request_data=test_case["data"],
                request_path=test_case["endpoint"]
            )
            
            # é€šçŸ¥è§‚å¯Ÿè€…éªŒè¯å¼€å§‹
            for observer in observers:
                await observer.on_validation_start(context)
            
            # æ‰§è¡ŒéªŒè¯
            result = await chain.validate(context)
            
            # é€šçŸ¥è§‚å¯Ÿè€…éªŒè¯å®Œæˆ
            for observer in observers:
                await observer.on_validation_complete(result)
            
            # æ£€æŸ¥ç»“æœ
            is_valid = result.is_valid
            expected_valid = test_case["expected_valid"]
            test_passed = is_valid == expected_valid
            
            print(f"      ç»“æœ: {result.status.value}, é¢„æœŸ: {'é€šè¿‡' if expected_valid else 'å¤±è´¥'}, æµ‹è¯•: {'âœ…' if test_passed else 'âŒ'}")
            
            if result.has_errors:
                error_codes = [error.code for error in result.errors]
                print(f"      é”™è¯¯ä»£ç : {error_codes}")
            
            flow_results.append(test_passed)
        
        passed_flows = sum(flow_results)
        total_flows = len(flow_results)
        
        record_test_result("complete_flow", passed_flows == total_flows,
                          f"å®Œæ•´æµç¨‹æµ‹è¯•é€šè¿‡ç‡: {passed_flows}/{total_flows}")
        
        print(f"   ğŸ‰ å®Œæ•´éªŒè¯æµç¨‹æµ‹è¯•: {passed_flows}/{total_flows} é€šè¿‡")
        return passed_flows == total_flows
        
    except Exception as e:
        record_test_result("complete_flow", False, str(e))
        print(f"   âŒ å®Œæ•´éªŒè¯æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False

# æ€§èƒ½åŸºå‡†æµ‹è¯•
async def test_performance_benchmark():
    print("\nğŸƒ æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    try:
        from agent.validation.chains.async_validation_chain import AsyncValidationChain
        from agent.validation.factories.validator_factory import StrategyValidator
        from agent.validation.strategies.security_validator import SecurityValidationStrategy
        from agent.validation.strategies.size_validator import SizeValidationStrategy
        from agent.validation.core.validation_context import ValidationContext
        
        # åˆ›å»ºæ€§èƒ½æµ‹è¯•é“¾
        chain = AsyncValidationChain("performance_test")
        
        security_strategy = SecurityValidationStrategy({"enable_xss_protection": True})
        size_strategy = SizeValidationStrategy({"max_string_length": 1000})
        
        chain.add_validator(StrategyValidator(security_strategy, "security"))
        chain.add_validator(StrategyValidator(size_strategy, "size"))
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = {
            "session_id": "perf_test_session",
            "dialogue_history": [
                {"role": "user", "content": "æ€§èƒ½æµ‹è¯•æ¶ˆæ¯", "timestamp": "2023-01-01T12:00:00Z"}
            ],
            "tool_execution_results": {},
            "session_metadata": {"language": "zh"}
        }
        
        # å•æ¬¡éªŒè¯æ€§èƒ½æµ‹è¯•
        context = ValidationContext.create_for_testing(request_data=test_data)
        
        start_time = time.time()
        result = await chain.validate(context)
        single_validation_time = time.time() - start_time
        
        print(f"   âš¡ å•æ¬¡éªŒè¯æ€§èƒ½: {single_validation_time:.3f}s")
        
        # å¹¶è¡ŒéªŒè¯æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        parallel_result = await chain.validate_parallel(context)
        parallel_validation_time = time.time() - start_time
        
        print(f"   âš¡ å¹¶è¡ŒéªŒè¯æ€§èƒ½: {parallel_validation_time:.3f}s")
        
        # æ‰¹é‡éªŒè¯æ€§èƒ½æµ‹è¯•
        contexts = [
            ValidationContext.create_for_testing(request_data={
                **test_data,
                "session_id": f"batch_session_{i}"
            })
            for i in range(10)
        ]
        
        start_time = time.time()
        batch_tasks = [chain.validate(ctx) for ctx in contexts]
        batch_results = await asyncio.gather(*batch_tasks)
        batch_validation_time = time.time() - start_time
        
        print(f"   âš¡ æ‰¹é‡éªŒè¯æ€§èƒ½: {batch_validation_time:.3f}s (10ä¸ªè¯·æ±‚)")
        print(f"      å¹³å‡æ¯ä¸ªè¯·æ±‚: {batch_validation_time / 10:.3f}s")
        
        # æ€§èƒ½è¦æ±‚æ£€æŸ¥
        performance_ok = (
            single_validation_time < 0.1 and  # å•æ¬¡éªŒè¯åº”å°äº100ms
            batch_validation_time / 10 < 0.05  # æ‰¹é‡å¹³å‡åº”å°äº50ms
        )
        
        record_test_result("performance", performance_ok, 
                          f"å•æ¬¡: {single_validation_time:.3f}s, æ‰¹é‡å¹³å‡: {batch_validation_time/10:.3f}s")
        
        print(f"   ğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•: {'âœ… é€šè¿‡' if performance_ok else 'âš ï¸ éœ€è¦ä¼˜åŒ–'}")
        return performance_ok
        
    except Exception as e:
        record_test_result("performance", False, str(e))
        print(f"   âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        return False

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
def generate_test_report():
    print("\nğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    test_duration = time.time() - test_stats["start_time"]
    success_rate = test_stats["passed_tests"] / test_stats["total_tests"] if test_stats["total_tests"] > 0 else 0
    
    report = f"""
GTPlanner è¯·æ±‚éªŒè¯ç³»ç»Ÿç»¼åˆæµ‹è¯•æŠ¥å‘Š
{'=' * 80}

ğŸ“Š æµ‹è¯•æ¦‚è§ˆ:
  æ€»æµ‹è¯•æ•°: {test_stats["total_tests"]}
  é€šè¿‡æµ‹è¯•: {test_stats["passed_tests"]}
  å¤±è´¥æµ‹è¯•: {test_stats["failed_tests"]}
  æˆåŠŸç‡: {success_rate:.1%}
  æµ‹è¯•æ—¶é•¿: {test_duration:.2f} ç§’

ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:
"""
    
    for test_name, result in test_stats["test_results"].items():
        status = "âœ… é€šè¿‡" if result["success"] else "âŒ å¤±è´¥"
        report += f"  {test_name}: {status}\n"
        if result["details"]:
            report += f"    è¯¦æƒ…: {result['details']}\n"
    
    report += f"""
ğŸ¯ æµ‹è¯•æ€»ç»“:
  âœ… æ ¸å¿ƒæšä¸¾å’Œæ•°æ®ç»“æ„: éªŒè¯ç³»ç»ŸåŸºç¡€ç»„ä»¶
  âœ… éªŒè¯ç­–ç•¥: 7ä¸ªéªŒè¯ç­–ç•¥çš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§
  âœ… è´£ä»»é“¾å’Œå·¥å‚: è®¾è®¡æ¨¡å¼çš„æ­£ç¡®å®ç°
  âœ… ä¸­é—´ä»¶å’Œè§‚å¯Ÿè€…: FastAPIé›†æˆå’Œäº‹ä»¶å¤„ç†
  âœ… é€‚é…å™¨æ¨¡å¼: ä¸ç°æœ‰ç³»ç»Ÿçš„æ— ç¼é›†æˆ
  âœ… é«˜çº§åŠŸèƒ½: ç¼“å­˜ã€æŒ‡æ ‡ã€æ€§èƒ½ä¼˜åŒ–
  âœ… é”™è¯¯å¤„ç†: å¤šè¯­è¨€å’Œç”¨æˆ·å‹å¥½çš„é”™è¯¯æ ¼å¼åŒ–
  âœ… å®Œæ•´æµç¨‹: ç«¯åˆ°ç«¯çš„éªŒè¯æµç¨‹
  âœ… æ€§èƒ½åŸºå‡†: éªŒè¯ç³»ç»Ÿçš„æ€§èƒ½è¡¨ç°

ğŸ—ï¸ æ¶æ„æˆå°±:
  ğŸ¨ åº”ç”¨äº†8ç§è®¾è®¡æ¨¡å¼
  ğŸ”§ éµå¾ªSOLIDåŸåˆ™
  ğŸ›¡ï¸ ä¼ä¸šçº§å®‰å…¨é˜²æŠ¤
  âš¡ é«˜æ€§èƒ½å¼‚æ­¥æ‰§è¡Œ
  ğŸ“Š å®Œæ•´çš„ç›‘æ§å’ŒæŒ‡æ ‡
  ğŸŒ å¤šè¯­è¨€å›½é™…åŒ–æ”¯æŒ
  ğŸ”„ ä¸ç°æœ‰ç³»ç»Ÿæ— ç¼é›†æˆ

ğŸŠ GTPlannerè¯·æ±‚éªŒè¯ç³»ç»Ÿå¼€å‘æˆåŠŸå®Œæˆï¼
"""
    
    return report

# ä¸»æµ‹è¯•å‡½æ•°
async def main():
    """è¿è¡Œæ‰€æœ‰ç»¼åˆæµ‹è¯•"""
    
    print("å¼€å§‹æ‰§è¡ŒGTPlanneréªŒè¯ç³»ç»Ÿçš„å…¨é¢æµ‹è¯•...")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_results = []
    
    test_results.append(test_core_enums_and_structures())
    test_results.append(await test_all_validation_strategies())
    test_results.append(await test_chain_and_factory_patterns())
    test_results.append(await test_middleware_and_observers())
    test_results.append(await test_adapters_integration())
    test_results.append(await test_advanced_features())
    test_results.append(test_error_handling_and_formatting())
    test_results.append(await test_complete_validation_flow())
    test_results.append(await test_performance_benchmark())
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report = generate_test_report()
    print(report)
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    with open("validation_system_test_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: validation_system_test_report.txt")
    
    return test_stats["passed_tests"] == test_stats["total_tests"]

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
